{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "modern-calendar",
   "metadata": {},
   "source": [
    "### Instituto Tecnologico de Costa Rica (ITCR)\n",
    "### Sede Interuniversitaria de Alajuela\n",
    "### Escuela de Computacion\n",
    "### Curso: Inteligencia Artificial\n",
    "### Estudiantes: \n",
    "\n",
    " - Brandon Ledezma Fernández - 2018185574\n",
    " - Walter Morales Vásquez - 2018212846\n",
    "\n",
    "### Profesora:\n",
    " \n",
    " - Maria Auxiliadora Mora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-soccer",
   "metadata": {},
   "source": [
    "# Tarea Programada Número 4\n",
    "---\n",
    "#### Introducción:\n",
    "En este trabajo práctico se aplicarán conceptos básicos de aprendizaje automático\n",
    "utilizando redes neuronales recurrentes para resolver problemas que involucran el\n",
    "procesamiento de lenguaje natural.\n",
    "\n",
    "El o los estudiantes deberán realizar dos ejercicios. El primero consiste en implementar\n",
    "una red neuronal recurrente aplicada a un problema de clasificación de textos de opinión\n",
    "sobre prendas de vestir de mujer. El segundo ejercicio consiste en reconocer nombres de\n",
    "entidades en textos (NER, Named-Entity Recognition)\n",
    "\n",
    "El objetivo del trabajo es poner en práctica las habilidades de investigación y el\n",
    "conocimiento adquirido durante el curso sobre redes neuronales por medio de\n",
    "ejercicios prácticos que permitan al estudiante experimentar con el aprendizaje profundo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "unavailable-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ner_evaluation.ner_eval import Evaluator\n",
    "from ner_evaluation.ner_eval import collect_named_entities\n",
    "from ner_evaluation.ner_eval import compute_metrics\n",
    "from ner_evaluation.ner_eval import compute_precision_recall_wrapper\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-section",
   "metadata": {},
   "source": [
    "## A. Clasificación   de   textos   con   redes   neuronales   recurrentes  LSTM utilizando Pytorch.\n",
    "\n",
    "Se desea que, dado un comentario de revisión de una prenda de vestir, predecir la calificación dada por el comprador. La calificación toma valores enteros entre 1 y 5, donde 1 corresponde a la peor calificación y 5 a la mejor.\n",
    "\n",
    "Datos: Utilice los datos de evaluación de prendas de vestir de mujer disponibles en Kaggle (nicapotato, 2018) para:\n",
    "\n",
    "1. Cargue y prepare los datos para ser introducidos a la red LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "surrounded-badge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23486.000000</td>\n",
       "      <td>23486.000000</td>\n",
       "      <td>23486.000000</td>\n",
       "      <td>23486.000000</td>\n",
       "      <td>23486.000000</td>\n",
       "      <td>23486.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11742.500000</td>\n",
       "      <td>918.118709</td>\n",
       "      <td>43.198544</td>\n",
       "      <td>4.196032</td>\n",
       "      <td>0.822362</td>\n",
       "      <td>2.535936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6779.968547</td>\n",
       "      <td>203.298980</td>\n",
       "      <td>12.279544</td>\n",
       "      <td>1.110031</td>\n",
       "      <td>0.382216</td>\n",
       "      <td>5.702202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5871.250000</td>\n",
       "      <td>861.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11742.500000</td>\n",
       "      <td>936.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>17613.750000</td>\n",
       "      <td>1078.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>23485.000000</td>\n",
       "      <td>1205.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>122.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0   Clothing ID           Age        Rating  \\\n",
       "count  23486.000000  23486.000000  23486.000000  23486.000000   \n",
       "mean   11742.500000    918.118709     43.198544      4.196032   \n",
       "std     6779.968547    203.298980     12.279544      1.110031   \n",
       "min        0.000000      0.000000     18.000000      1.000000   \n",
       "25%     5871.250000    861.000000     34.000000      4.000000   \n",
       "50%    11742.500000    936.000000     41.000000      5.000000   \n",
       "75%    17613.750000   1078.000000     52.000000      5.000000   \n",
       "max    23485.000000   1205.000000     99.000000      5.000000   \n",
       "\n",
       "       Recommended IND  Positive Feedback Count  \n",
       "count     23486.000000             23486.000000  \n",
       "mean          0.822362                 2.535936  \n",
       "std           0.382216                 5.702202  \n",
       "min           0.000000                 0.000000  \n",
       "25%           1.000000                 0.000000  \n",
       "50%           1.000000                 1.000000  \n",
       "75%           1.000000                 3.000000  \n",
       "max           1.000000               122.000000  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nRowsRead = 1000\n",
    "\n",
    "# Se carga el archivo con los datos solicitados (defaultofcredit.csv) y se define\n",
    "# a la columna \"default_payment_next_month\" como la objetivo.\n",
    "reviews = pd.read_csv('./data/Womens Clothing E-Commerce Reviews.csv', delimiter=',')#, nrows=nRowsRead)\n",
    "#reviews = pd.read_csv('./data/a.csv')\n",
    "reviews.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "cd7d8590",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how does\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\" u \": \" you \",\n",
    "\" ur \": \" your \",\n",
    "\" n \": \" and \"}\n",
    "\n",
    "def cont_to_exp(x):\n",
    "    if type(x) is str:\n",
    "        x = x.replace('\\\\', '')\n",
    "        for key in contractions:\n",
    "            value = contractions[key]\n",
    "            x = x.replace(key, value)\n",
    "        return x\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "reviews['Review Text'] = reviews['Review Text'].apply(lambda x: cont_to_exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6c07d33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 821\n",
      "2 1549\n",
      "3 2823\n",
      "4 4908\n",
      "5 12540\n"
     ]
    }
   ],
   "source": [
    "# Delete missing observations for following variables\n",
    "# for x in [\"Clothing ID\",\"Age\",\"Title\",\"Review Text\",\"Rating\",\"Recommended IND\",\"Positive Feedback Count\",\"Division Name\",\"Department Name\",\"Class Name\"]:\n",
    "#     reviews = reviews[reviews[x].notnull()]\n",
    "    \n",
    "# X = reviews.drop(columns = reviews.columns[3:5])\n",
    "# X = pd.get_dummies(X)\n",
    "# y = reviews['Review Text']\n",
    "# y = [elem.split() for elem in y]\n",
    "\n",
    "#reviews['Rating'] = reviews[(reviews['Rating'] >= 1)  (reviews['Rating'] <= 5)]\n",
    "\n",
    "reviews = reviews[reviews[\"Review Text\"].notnull()]\n",
    "reviews = reviews[reviews[\"Rating\"].notnull()]\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "def preprocesar_texto(texto):\n",
    "    \n",
    "    texto = texto.lower()\n",
    "    texto = \"\".join([ch if ch not in punctuation else \" \" for ch in texto])\n",
    "#     texto = \"\".join([ch for ch in texto if ch not in punctuation])\n",
    "    return texto\n",
    "\n",
    "# reviews['Review Text'] = reviews['Review Text'].apply(lambda x: preprocesar_texto(x))\n",
    "\n",
    "X = reviews['Review Text']#.tolist()\n",
    "X = X.tolist()\n",
    "\n",
    "for i in range(len(X)):\n",
    "    X[i] = preprocesar_texto(X[i])\n",
    "\n",
    "#print(X)\n",
    "\n",
    "X = [elem.split() for elem in X]\n",
    "\n",
    "# for i in range(len(X)):\n",
    "#     temp = []\n",
    "#     for j in range(len(X[i])):\n",
    "#         temp.append(preprocesar_texto(X[i][j]))\n",
    "#     X[i] = temp\n",
    "\n",
    "y = reviews['Rating']#.tolist()\n",
    "\n",
    "print(1, y.tolist().count(1))\n",
    "print(2, y.tolist().count(2))\n",
    "print(3, y.tolist().count(3))\n",
    "print(4, y.tolist().count(4))\n",
    "print(5, y.tolist().count(5))\n",
    "\n",
    "word_to_ix = {\"\":0}\n",
    "\n",
    "for review in X:\n",
    "#     print(review)\n",
    "#     if review not in word_to_ix:\n",
    "#         word_to_ix[review] = len(word_to_ix)\n",
    "    for word in review:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "ratings = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "democratic-cable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "14168\n",
      "[[[1, 2, 3, 4, 5, 4, 6], 7], [[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 10, 18, 19, 20, 4, 14, 21, 22, 14, 23, 24, 14, 25, 26, 27, 28, 10, 29, 24, 10, 11, 30, 14, 31, 19, 30, 4, 21, 32, 33, 14, 7, 34, 35, 36, 37, 38, 39, 19, 40, 41, 34, 42, 26, 43, 44, 19, 45, 46, 36, 47, 48, 11, 49, 30], 66], [[14, 50, 51, 52, 53, 54, 8, 9, 4, 55, 56, 10, 16, 57, 54, 37, 14, 58, 28, 34, 30, 59, 60, 61, 62, 63, 14, 64, 8, 16, 44, 65, 59, 66, 59, 18, 67, 68, 14, 69, 70, 71, 10, 72, 14, 73, 10, 18, 30, 74, 75, 76, 39, 77, 78, 34, 79, 80, 76, 6, 4, 81, 82, 63, 34, 83, 80, 50, 19, 84, 85, 86, 87, 4, 88, 89, 90, 91, 92, 93, 94, 19, 95, 96, 97, 76, 34, 91, 92, 87, 98, 99, 100, 34, 101, 10, 102], 97], [[14, 7, 7, 7, 8, 103, 10, 11, 104, 105, 4, 106, 107, 108, 14, 109, 10, 14, 110, 111, 63, 112, 113], 23], [[8, 114, 11, 84, 115, 16, 116, 117, 16, 34, 118, 119, 120, 10, 11, 34, 121, 35, 16, 109, 122, 123, 4, 10, 11, 124, 66, 10, 125, 126, 122, 127, 128, 7, 8, 114], 36]]\n"
     ]
    }
   ],
   "source": [
    "max_lenght = 0\n",
    "for elem in X:\n",
    "    if max_lenght < len(elem):\n",
    "        max_lenght = len(elem)\n",
    "\n",
    "        \n",
    "# for i in range(len(X)):\n",
    "#     largo = len(X[i])\n",
    "#     if largo < max_lenght:\n",
    "#         X[i] += ['<pad>']*(max_lenght-largo)\n",
    "# print(unique)\n",
    "\n",
    "print(max_lenght)\n",
    "\n",
    "palabras_diferentes = set([elem for row in X for elem in row])\n",
    "cantidad_palabras = len(palabras_diferentes)\n",
    "\n",
    "print(cantidad_palabras)\n",
    "\n",
    "#print(palabras_diferentes)\n",
    "# palabras_diferentes\n",
    "# np.savetxt('output.txt', arreglo, delimiter=',')\n",
    "# X\n",
    "# palabras_diferentes\n",
    "# Preparación de los datos \n",
    "def prepare_sequence(seq, to_ix):\n",
    "    # Prepara tensores de indices de palabras a partir de una oración.\n",
    "    # Parámetros:\n",
    "    #   seq: oración\n",
    "    #   to_ix: diccionario de palabras.\n",
    "#     print(seq)\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    #idxs = to_ix[seq]\n",
    "    return idxs\n",
    "\n",
    "X_procesada = []\n",
    "for i in range(len(X)):\n",
    "    temp = prepare_sequence(X[i], word_to_ix)\n",
    "    X_procesada.append([temp, len(temp)])\n",
    "    \n",
    "print(X_procesada[:5])\n",
    "    \n",
    "# def padding_(sentences, seq_len):\n",
    "#     features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "#     for ii, review in enumerate(sentences):\n",
    "#         if len(review) != 0:\n",
    "#             features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "#     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3e9a41cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 9, 18, 19, 693, 14168, 11, 239, 4, 273, 261, 1172, 16, 109, 4, 583, 248, 1894, 747], 19]\n",
      "[[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 10, 18, 19, 20, 4, 14, 21, 22, 14, 23, 24, 14, 25, 26, 27, 28, 10, 29, 24, 10, 11, 30, 14, 31, 19, 30, 4, 21, 32, 33, 14, 7, 34, 35, 36, 37, 38, 39, 19, 40, 41, 34, 42, 26, 43, 44, 19, 45, 46, 36, 47, 48, 11, 49, 30, 0, 0, 0, 0], 66]\n"
     ]
    }
   ],
   "source": [
    "print(X_procesada[-1])\n",
    "\n",
    "def padding_(sentences, seq_len):\n",
    "    features = [[0]*seq_len for i in range(len(sentences))]#np.zeros((len(sentences), seq_len), dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        largo = review[1]\n",
    "        if largo != 0:\n",
    "            features[ii][:largo] = np.array(review[0])[:seq_len]\n",
    "            features[ii] = [features[ii], largo]\n",
    "#             features[ii] = np.concatenate((features[ii], [largo]), axis=0)# = features[ii] + [largo]\n",
    "    return features\n",
    "\n",
    "X_procesada = padding_(X_procesada, 70)# Promedio de largo #max_lenght)\n",
    "print(X_procesada[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ba34c9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    }
   ],
   "source": [
    "print(max_lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-likelihood",
   "metadata": {},
   "source": [
    "2. Utilizando PyTorch defina una red recurrente LSTM para procesar el conjunto de datos y clasificar los comentarios de usuario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "intimate-number",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor([[1, 1],\n",
      "        [3, 3]])\n",
      "tensor([[2., 2.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def tensor_mean(input):\n",
    "    \n",
    "    #output = torch.tensor(np.zeros(input[0].size(0)))\n",
    "    print('input', input)\n",
    "    #input?\n",
    "    output = input.clone().detach()\n",
    "    #output.grad_fn.data.copy_(input.grad_fn.data)\n",
    "    \n",
    "    input = input[torch.arange(input.size(0))==0].double()\n",
    "#     print('output', output)\n",
    "#     print('input', input)\n",
    "#     print('output', output)\n",
    "#     print('output', output[0])\n",
    "#     print('input[0].size(0)', input[0].size(0))\n",
    "    \n",
    "    for i in range(1, output.size(0)):\n",
    "        for j in range(output[i].size(0)):\n",
    "            input[0][j] += output[i][j]\n",
    "            \n",
    "    for i in range(input[0].size(0)):\n",
    "        input[0][i] /= output.size(0)\n",
    "    return input\n",
    "    #return torch.tensor([output.tolist()], grad_fn=input.grad_fn)#requires_grad=False) grad_fn=input.grad\n",
    "\n",
    "# Funciones utilitarias\n",
    "\n",
    "def max_values(x):\n",
    "    # Retorna el valor máximo y en índice o la posición del valor en un vector x.\n",
    "    # Parámetros: \n",
    "    #    x: vector con los datos. \n",
    "    # Salida: \n",
    "    #    out: valor \n",
    "    #    inds: índice\n",
    "    out, inds = torch.max(x,dim=1)   \n",
    "    return out, inds\n",
    "\n",
    "print(tensor_mean(torch.tensor([[1,1],[3,3]])))\n",
    "#max_values(torch.tensor([[1,2,1,5,9]]))\n",
    "\n",
    "def clone_vs_deepcopy(x):\n",
    "    import copy\n",
    "    import torch\n",
    "\n",
    "    x_clone = x.clone()\n",
    "    x_deep_copy = copy.deepcopy(x)\n",
    "    #\n",
    "    x.mul_(-1)\n",
    "    print(f'x = {x}')\n",
    "    print(f'x_clone = {x_clone}')\n",
    "    print(f'x_deep_copy = {x_deep_copy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2fc3e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 5)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])\n",
    "\n",
    "batch_size = 150# No es mucho?\n",
    "vocab_size = len(word_to_ix)\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 50\n",
    "    \n",
    "# Instancia del modelo\n",
    "model = LSTM(vocab_size, EMBEDDING_DIM, HIDDEN_DIM) # len([1,2,3,4,5]))\n",
    "\n",
    "# Función de pérdida: Negative Log Likelihood Loss (NLLL). \n",
    "# Útil para problemas de clasificacion con C clases.\n",
    "# loss_function = F.cross_entropy(y_pred, y)#nn.NLLLoss()\n",
    "\n",
    "# Optimizador Stochastic Gradient Descent  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)#SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "# Definición del modelo\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "approximate-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # Incialización del modelo\n",
    "# #     def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "# #         super(LSTMTagger, self).__init__()\n",
    "# #         self.hidden_dim = hidden_dim\n",
    "\n",
    "# #         # Primero se pasa la entrada a través de una capa Embedding. \n",
    "# #         # Esta capa construye una representación de los tokens de \n",
    "# #         # un texto donde las palabras que tienen el mismo significado \n",
    "# #         # tienen una representación similar.\n",
    "        \n",
    "# #         # Esta capa captura mejor el contexto y son espacialmente \n",
    "# #         # más eficientes que las representaciones vectoriales (one-hot vector).\n",
    "# #         # En Pytorch, se usa el módulo nn.Embedding para crear esta capa, \n",
    "# #         # que toma el tamaño del vocabulario y la longitud deseada del vector \n",
    "# #         # de palabras como entrada. \n",
    "# #         self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# #         # El LSTM toma word_embeddings como entrada y genera estados ocultos\n",
    "# #         # con dimensionalidad hidden_dim.  \n",
    "# #         self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "# #         # La capa lineal mapea el espacio de estado oculto \n",
    "# #         # al espacio de etiquetas\n",
    "# #         self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "#     def forward(self,x,hidden):\n",
    "#         batch_size = x.size(0)\n",
    "#         # embeddings and lstm_out\n",
    "#         embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "#         #print(embeds.shape)  #[50, 500, 1000]\n",
    "#         lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "#         lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "#         # dropout and fully connected layer\n",
    "#         out = self.dropout(lstm_out)\n",
    "#         out = self.fc(out)\n",
    "        \n",
    "#         # sigmoid function\n",
    "#         sig_out = self.sig(out)\n",
    "        \n",
    "#         # reshape to be batch_size first\n",
    "#         sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "#         sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "#         # return last sigmoid output and hidden state\n",
    "#         return sig_out, hidden\n",
    "    \n",
    "# #     def forward(self, sentence):\n",
    "        \n",
    "# #         # Pase hacia adelante de la red. \n",
    "# #         # Parámetros:\n",
    "# #         #    sentence: la oración a procesar\n",
    "# #         embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "# #         print('embeds', embeds)\n",
    "# #         lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        \n",
    "# #         print('lstm_out', lstm_out)\n",
    "# #         tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "# #         print('tag_space', tag_space)\n",
    "        \n",
    "# #         # Se utiliza softmax para devolver un peso por etiqueta\n",
    "# #         tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        \n",
    "# #         return tag_scores#torch.mean(tag_scores,0)\n",
    "\n",
    "# # Instanciación del modelo, definición de la función de pérdida y del optimizador   \n",
    "\n",
    "# # Hiperparámetros de la red\n",
    "# # Valores generalmente altos (32 o 64 dimensiones).\n",
    "# # Se definen pequeños, para ver cómo cambian los pesos durante el entrenamiento.\n",
    "\n",
    "\n",
    "# EMBEDDING_DIM = 64\n",
    "# HIDDEN_DIM = 256\n",
    "\n",
    "# # Instancia del modelo\n",
    "# model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(ratings)) # len([1,2,3,4,5]))\n",
    "\n",
    "# # Función de pérdida: Negative Log Likelihood Loss (NLLL). \n",
    "# # Útil para problemas de clasificacion con C clases.\n",
    "# loss_function = nn.NLLLoss()\n",
    "\n",
    "# # Optimizador Stochastic Gradient Descent  \n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1ccf511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo\n",
    "\n",
    "# El modelo es una clase que debe heredar de nn.Module\n",
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.5):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.n_hidden = n_hidden   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        sigmoid_last = sigmoid_out[:, -1]               # (batch_size, 1)\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h\n",
    "\n",
    "# Instanciación del modelo, definición de la función de pérdida y del optimizador   \n",
    "\n",
    "# Hiperparámetros de la red\n",
    "# Valores generalmente altos (32 o 64 dimensiones).\n",
    "# Se definen pequeños, para ver cómo cambian los pesos durante el entrenamiento.\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 256\n",
    "n_layers = 2\n",
    "\n",
    "# Instancia del modelo\n",
    "# model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(ratings), n_layers) # len([1,2,3,4,5]))\n",
    "\n",
    "model = LSTMTagger(len(word_to_ix), EMBEDDING_DIM, HIDDEN_DIM, len(ratings), n_layers)\n",
    "\n",
    "# Función de pérdida: Negative Log Likelihood Loss (NLLL). \n",
    "# Útil para problemas de clasificacion con C clases.\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# Optimizador Stochastic Gradient Descent  \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-kentucky",
   "metadata": {},
   "source": [
    "3. Separe las muestras en datos de entrenamiento y evaluación y entrene el modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "traditional-excitement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print('X_procesada', X_procesada)\n",
    "\n",
    "y = y.apply(lambda a: a-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        X_procesada,\n",
    "                                        y.values,\n",
    "                                        train_size   = 0.7,\n",
    "                                        random_state = 1234,\n",
    "                                        shuffle      = False\n",
    "                                    )\n",
    "\n",
    "\n",
    "\n",
    "class ReviewsDataset(TensorDataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx][0]), self.y[idx], self.X[idx][1]\n",
    "\n",
    "# create Tensor datasets\n",
    "# print('X_train', X_train)\n",
    "# print('y_train', y_train)\n",
    "train_data = ReviewsDataset(X_train, y_train)#TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "valid_data = ReviewsDataset(X_test, y_test)#TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "# dataloaders\n",
    "#batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# print('X_train', X_train.values)\n",
    "# print('X_test', X_test.values)\n",
    "# print('y_train', y_train)\n",
    "# print('y_test', y_test)\n",
    "\n",
    "#print('X.values.tolist()', X.values.tolist())\n",
    "\n",
    "#test = torch.tensor(y.values.tolist())\n",
    "\n",
    "# X_train = torch.tensor(X_train.values.astype(np.float32)) \n",
    "# X_test = torch.tensor(X_test.values.astype(np.float32)) \n",
    "\n",
    "# y_train = torch.tensor(y_train.values.astype(np.float32))\n",
    "# y_test = torch.tensor(y_test.values.astype(np.float32))\n",
    "\n",
    "# train_target = torch.tensor(train['Target'].values.astype(np.float32))\n",
    "# train = torch.tensor(train.drop('Target', axis = 1).values.astype(np.float32)) \n",
    "# train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "# train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "print(set(y_test))\n",
    "\n",
    "print(X_train[0], y_train[0])\n",
    "# word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "962e7be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "print(set(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ceramic-words",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.243, val loss 1.298, val accuracy 0.539, and val rmse 1.398\n"
     ]
    }
   ],
   "source": [
    "# # Preparación de los datos \n",
    "# def prepare_sequence(seq, to_ix):\n",
    "#     # Prepara tensores de indices de palabras a partir de una oración.\n",
    "#     # Parámetros:\n",
    "#     #   seq: oración\n",
    "#     #   to_ix: diccionario de palabras.\n",
    "#     idxs = [to_ix[w] for w in seq]\n",
    "#     #idxs = to_ix[seq]\n",
    "#     return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# Entrenar el modelo \n",
    "\n",
    "# Valores antes de entrenar\n",
    "# El elemento i, j de la salida es la puntuación entre la etiqueta j para la palabra i.\n",
    "# with torch.no_grad():\n",
    "#     inputs = X_train[0]#prepare_sequence(X_train[0], word_to_ix)\n",
    "#     print('inputs', inputs)\n",
    "#     tag_scores = model(inputs)\n",
    "    \n",
    "#     print(X_train[0])\n",
    "    \n",
    "#     # Clasificación    \n",
    "#     print(tag_scores)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def validation_metrics (model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    predicciones = []\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        y_hat = model(x, l)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "        predicciones += pred\n",
    "    return sum_loss/total, correct/total, sum_rmse/total, predicciones\n",
    "\n",
    "epocas = 5\n",
    "# Corridas o épocas\n",
    "for i in range(epocas):\n",
    "    model.train()\n",
    "    sum_loss = 0.0\n",
    "    total = 0\n",
    "    for x, y, l in train_loader:\n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        y_pred = model(x, l)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        total += y.shape[0]\n",
    "    val_loss, val_acc, val_rmse, _ = validation_metrics(model, valid_loader)\n",
    "    if i % 5 == 1:\n",
    "        print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
    "\n",
    "\n",
    "# Despligue de la puntuación luego del entrenamiento\n",
    "# with torch.no_grad():\n",
    "#     inputs = prepare_sequence(training_data[0], word_to_ix)\n",
    "#     tag_scores = model(inputs)\n",
    "   \n",
    "#     print(\"Resultados luego del entrenamiento para la primera frase\")\n",
    "#     # Las palabras en una oración se pueden etiquetar de tres formas.\n",
    "#     # La primera oración tiene 4 palabras \"El perro come manzana\"\n",
    "#     # por eso el tensor de salida tiene 4 elementos. \n",
    "#     # Cada elemento es un vector de pesos que indica cuál etiqueta tiene más\n",
    "#     # posibilidad de estar asociada a la palabra. Es decir hay que calcular \n",
    "#     # la posición del valor máximo\n",
    "#     print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "af898359",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-326-e8861331a23f>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-326-e8861331a23f>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    for inputs, labels in [X_train[:10], y_train[:10]]#train_loader:\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "print_every = 10\n",
    "step = 0\n",
    "n_epochs = 1  # validation loss increases from ~ epoch 3 or 4\n",
    "clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in [X_train[:10], y_train[:10]]:#train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # making requires_grad = False for the latest set of h\n",
    "        h = tuple([each.data for each in h])   \n",
    "        \n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs)\n",
    "        \n",
    "#         print('output.squeeze()', output.squeeze())\n",
    "#         print('labels.float()', labels.T[0].float())\n",
    "        \n",
    "        loss = criterion(output.squeeze(), labels.T[0].float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            ######################\n",
    "            ##### VALIDATION #####\n",
    "            ######################\n",
    "            model.eval()\n",
    "            valid_losses = []\n",
    "            v_h = model.init_hidden(batch_size)\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "                v_h = tuple([each.data for each in v_h])\n",
    "                \n",
    "                v_output, v_h = model(v_inputs)\n",
    "                v_loss = criterion(output.squeeze(), labels.T[0].float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-pipeline",
   "metadata": {},
   "source": [
    "4. Evalúe el modelo resultante utilizando una matriz de confusión y métricas extraídas a partir de esta (ie. precisión, exhaustividad y F1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "abstract-active",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-27 20:36:49 matplotlib.colorbar DEBUG: locator: <matplotlib.colorbar._ColorbarAutoLocator object at 0x7fab5a82f9d0>\n",
      "2021-05-27 20:36:49 matplotlib.colorbar DEBUG: Using auto colorbar locator <matplotlib.colorbar._ColorbarAutoLocator object at 0x7fab5a82f9d0> on colorbar\n",
      "2021-05-27 20:36:49 matplotlib.colorbar DEBUG: Setting pcolormesh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.246, val loss 1.319, val accuracy 0.534, and val rmse 1.386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.25      0.00      0.01       262\n",
      "           2       0.00      0.00      0.00       479\n",
      "           3       0.18      0.01      0.02       850\n",
      "           4       0.21      0.02      0.04      1514\n",
      "           5       0.55      0.97      0.70      3688\n",
      "\n",
      "    accuracy                           0.53      6793\n",
      "   macro avg       0.24      0.20      0.15      6793\n",
      "weighted avg       0.37      0.53      0.39      6793\n",
      "\n",
      "Confusion Matrix : \n",
      "[[   1    1    0    2    0]\n",
      " [   0    0    3    0    1]\n",
      " [   4    3    9   12   21]\n",
      " [   8   16   23   33   80]\n",
      " [ 249  459  815 1467 3586]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "val_loss, val_acc, val_rmse, predicciones = validation_metrics(model, valid_loader)\n",
    "print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "# se muestran los resultados para comparar con los obtenidos\n",
    "print(classification_report(y_test, predicciones, target_names=[\"1\", \"2\", \"3\", \"4\", \"5\"]))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion Matrix : \")\n",
    "cm = confusion_matrix(predicciones, y_test)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in \"12345\"],\n",
    "                  columns = [i for i in \"12345\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-aaron",
   "metadata": {},
   "source": [
    "5. Genere y documente sus conclusiones (incluya al menos cuatro conclusiones importantes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb079b09",
   "metadata": {},
   "source": [
    "Importancia de meterle los pads, sin estos el modelo\n",
    "\n",
    "Importancia de limpiar los textos, reducción en la cantidad de palabras únicas, \n",
    "\n",
    "LSTM appears to be theoretically involved, but its Pytorch implementation is pretty straightforward. Also, while looking at any problem, it is very important to choose the right metric, in our case if we’d gone for accuracy, the model seems to be doing a very bad job, but the RMSE shows that it is off by less than 1 rating point, which is comparable to human performance!\n",
    "\n",
    "LSTM appears to be theoretically involved, but its Pytorch implementation is pretty straightforward. Also, while looking at any problem, it is very important to choose the right metric, in our case if we’d gone for accuracy, the model seems to be doing a very bad job, but the RMSE shows that it is off by less than 1 rating point, which is comparable to human performance!\n",
    "\n",
    "Problema bastante dificil, incluso para los humanos\n",
    "\n",
    "Pocas reviews de bajas estrellas (de baja puntuación), mientras que muchas de alta, lo que conlleva a que nuestro modelo se vea alterado y pueda identificar (trabajar) mejor las de 5\n",
    "1 821\n",
    "2 1549\n",
    "3 2823\n",
    "4 4908\n",
    "5 12540"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-mission",
   "metadata": {},
   "source": [
    "Referencias\n",
    "----------------\n",
    "*\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/multiclass-text-classification-using-lstm-in-pytorch-eac56baed8df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-modem",
   "metadata": {},
   "source": [
    "## B. Reconocimiento   de   nombres   de   entidades   (NER,   Name   EntityRecognition) con redes neuronales recurrentes utilizando Pytorch.\n",
    "\n",
    "El reconocimiento de nombres de entidades (NER)  es  el proceso  de identificar y categorizar elementos clave (ej. entidades) en el texto. Una entidad puede ser cualquier palabra o secuencia de palabras que se refieren a una persona, animal, sitio o cosa (ej.empresa, región geográfica, objeto). Cada entidad detectada se clasifica en una categoría predeterminada. Normalmente, NER se aborda como un problema de etiquetado de secuencias. Una explicación muy detallada de porqué es importante extraer entidades de los textos se encuentra en (Monge, 2020).\n",
    "\n",
    "Los algoritmos de extracción de entidades pueden únicamente detectar la presencia de una entidad y marcarla como tal o pueden detectar y clasificar cada entidad que encuentran.\n",
    "\n",
    "Ejemplo: En una oración como “arbusto de 2 m. flores lila.”.  Cada palabra representa un token donde “arbusto” y “flores” son los elementos de interés a marcar. El etiquetado “token inicial- token interno” es una forma común de indicar dónde comienzan y terminan las entidades. En el ejemplo anterior la etiqueta sería “B O O O B O” donde B representael inicio de la entidad y O cualquier otro token. Para la oración “botones florales rosados.” la etiqueta estaría dada por “B I O” donde “B” marca el token inicio de la entidad e “I” los otros tokens que son parte de esta, es decir “B I” delimita la entidad “botones florales”. \n",
    "\n",
    "Otra forma de marcar y etiquetar, es además de delimitar la entidad, asignar a esta la clase a la que corresponde, por ejemplo: empresa, ciudad, persona, entre otros. Para el presente ejercicio se va a utilizar este enfoque.\n",
    "\n",
    "Utilice los datos para reconocer y clasificar nombres de entidades compartidos en Kaggle por (Ranjan, 2020) para:\n",
    "\n",
    "1. Cargue y prepare los datos para ser introducidos a la red recurrente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "internal-nothing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_ID</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>997</td>\n",
       "      <td>877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sentence: 10764</td>\n",
       "      <td>['It', 'is', 'written', 'in', 'brown', 'and', ...</td>\n",
       "      <td>['NNP', 'POS', 'NNP', 'NNP', 'VBZ', '.']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sentence_ID                                               Word  \\\n",
       "count              1000                                               1000   \n",
       "unique             1000                                               1000   \n",
       "top     Sentence: 10764  ['It', 'is', 'written', 'in', 'brown', 'and', ...   \n",
       "freq                  1                                                  1   \n",
       "\n",
       "                                             POS  \\\n",
       "count                                       1000   \n",
       "unique                                       997   \n",
       "top     ['NNP', 'POS', 'NNP', 'NNP', 'VBZ', '.']   \n",
       "freq                                           3   \n",
       "\n",
       "                                                      Tag  \n",
       "count                                                1000  \n",
       "unique                                                877  \n",
       "top     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "freq                                                   13  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nRowsRead = 1000\n",
    "\n",
    "# Se carga el archivo con los datos solicitados (defaultofcredit.csv) y se define\n",
    "# a la columna \"default_payment_next_month\" como la objetivo.\n",
    "ner = pd.read_csv('./data/NER_Dataset.csv', delimiter=',', nrows=nRowsRead)\n",
    "\n",
    "ner.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "corporate-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete missing observations for following variables\n",
    "# for x in [\"Clothing ID\",\"Age\",\"Title\",\"Review Text\",\"Rating\",\"Recommended IND\",\"Positive Feedback Count\",\"Division Name\",\"Department Name\",\"Class Name\"]:\n",
    "#     reviews = reviews[reviews[x].notnull()]\n",
    "    \n",
    "# X = reviews.drop(columns = reviews.columns[3:5])\n",
    "# X = pd.get_dummies(X)\n",
    "# y = reviews['Review Text']\n",
    "# y = [elem.split() for elem in y]\n",
    "\n",
    "#reviews['Rating'] = reviews[(reviews['Rating'] >= 1)  (reviews['Rating'] <= 5)]\n",
    "\n",
    "\n",
    "# ner[\"Word\"] = ner[\"Word\"].str.replace(r\"'\", '', regex=True)\n",
    "# ner[\"Word\"] = ner[\"Word\"].str.replace(r\"[\", '', regex=True)\n",
    "# ner[\"Word\"] = ner[\"Word\"].str.replace(r\"]\", '', regex=True)\n",
    "# ner[\"Word\"] = ner[\"Word\"].str.replace(r\" \", '', regex=True)\n",
    "\n",
    "# ner[\"Word\"] = ner[\"Word\"].str.replace(r\"'\", '', regex=True)\n",
    "# ner[\"Word\"] = ner[\"Word\"].str.replace(r\"[\", '', regex=True)\n",
    "# ner[\"Word\"] = ner[\"Word\"].str.replace(r\"]\", '', regex=True)\n",
    "# ner[\"Word\"] = ner[\"Word\"].str.replace(r\" \", '', regex=True)\n",
    "\n",
    "#ner[\"Word\"] = ner[\"Word\"].Series.str.replace('[', '', regex=True)\n",
    "\n",
    "for x in [\"Word\", \"Tag\"]:\n",
    "#     ner[x] = ner[x].str.replace(\"[\", '', 1, regex=True)\n",
    "#     ner[x] = ner[x][-1].str.replace(\"]\", '', 1, regex=True)\n",
    "    ner[x] = ner[x].str.replace(\"'\", '', regex=True)\n",
    "    ner[x] = ner[x].str.replace(\", \", ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "concerned-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ner['Word']\n",
    "X = [elem[1:-1].split() for elem in X]\n",
    "# X[0] = X[0][1:]\n",
    "# X[-1] = X[0][1:]\n",
    "y = ner['Tag']\n",
    "y = [elem[1:-1].split() for elem in y]\n",
    "# y.pop(0)\n",
    "# y.pop()\n",
    "\n",
    "\n",
    "# def limpiar_datos(dataset):\n",
    "#     for i in range(len(dataset)):\n",
    "#         dataset[i].remove()\n",
    "    \n",
    "word_to_ix = {}\n",
    "for frase in X:\n",
    "#     if review not in word_to_ix:\n",
    "#         word_to_ix[review] = len(word_to_ix)\n",
    "    for palabra in frase:\n",
    "        if palabra not in word_to_ix:\n",
    "            word_to_ix[palabra] = len(word_to_ix)\n",
    "\n",
    "            \n",
    "etiquetas_a_indice = {'O':0,'B-geo':1,'B-gpe':2,'B-org':3,'B-nat':4,'B-art':5,'B-eve':6,'B-tim':7,'B-per':8,\n",
    "                      'I-geo':9,'I-gpe':10,'I-org':11,'I-nat':12,'I-art':13,'I-eve':14,'I-tim':15,'I-per':16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "plain-balloon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-org', 'I-eve', 'B-gpe', 'I-geo', 'I-gpe', 'B-tim', 'B-art', 'I-org', 'O', 'I-tim', 'B-eve', 'B-per', 'B-nat', 'I-art', 'B-geo', 'I-per', 'I-nat'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4783"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "test = flatten(y)\n",
    "# print(test)\n",
    "myset = set(test)\n",
    "print(myset)\n",
    "len(word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-happiness",
   "metadata": {},
   "source": [
    "2. Utilizando PyTorch defina una red recurrente LSTM para procesar, localizar y clasificar las entidades presentes en el texto (como la vista en clase). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "failing-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo\n",
    "\n",
    "# El modelo es una clase que debe heredar de nn.Module\n",
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    # Incialización del modelo\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "\n",
    "        # Primero se pasa la entrada a través de una capa Embedding. \n",
    "        # Esta capa construye una representación de los tokens de \n",
    "        # un texto donde las palabras que tienen el mismo significado \n",
    "        # tienen una representación similar.\n",
    "        \n",
    "        # Esta capa captura mejor el contexto y son espacialmente \n",
    "        # más eficientes que las representaciones vectoriales (one-hot vector).\n",
    "        # En Pytorch, se usa el módulo nn.Embedding para crear esta capa, \n",
    "        # que toma el tamaño del vocabulario y la longitud deseada del vector \n",
    "        # de palabras como entrada. \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # El LSTM toma word_embeddings como entrada y genera estados ocultos\n",
    "        # con dimensionalidad hidden_dim.  \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # La capa lineal mapea el espacio de estado oculto \n",
    "        # al espacio de etiquetas\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # Pase hacia adelante de la red. \n",
    "        # Parámetros:\n",
    "        #    sentence: la oración a procesar\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "\n",
    "#         print('tag_space', tag_space)\n",
    "        \n",
    "        # Se utiliza softmax para devolver un peso por etiqueta\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "#         print('tag_scores', tag_scores)\n",
    "        return tag_scores\n",
    "\n",
    "# Instanciación del modelo, definición de la función de pérdida y del optimizador   \n",
    "\n",
    "# Hiperparámetros de la red\n",
    "# Valores generalmente altos (32 o 64 dimensiones).\n",
    "# Se definen pequeños, para ver cómo cambian los pesos durante el entrenamiento.\n",
    "\n",
    "EMBEDDING_DIM = 6#34\n",
    "HIDDEN_DIM = 6#34\n",
    "\n",
    "# Instancia del modelo\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(etiquetas_a_indice)) # len([1,2,3,4,5]))\n",
    "\n",
    "# Función de pérdida: Negative Log Likelihood Loss (NLLL). \n",
    "# Útil para problemas de clasificacion con C clases.\n",
    "loss_function = nn.NLLLoss()#nn.CrossEntropyLoss()#nn.NLLLoss()\n",
    "\n",
    "# Optimizador Stochastic Gradient Descent  \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-walker",
   "metadata": {},
   "source": [
    "3. Separe las muestras en datos de entrenamiento y evaluación y entrene el modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "purple-afternoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train ['The', 'group', 'says', 'its', 'departure', 'will', 'affect', '3,00,000', 'people', 'in', 'South', 'Darfur', '.'] 700 19\n",
      "y_train ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'O'] 700 19\n"
     ]
    }
   ],
   "source": [
    "proporcion = lambda : 0.7\n",
    "\n",
    "# Se randomizan las posiciones de las clases.\n",
    "random.shuffle(X, proporcion)\n",
    "random.shuffle(y, proporcion)\n",
    "\n",
    "indice = int(len(X)*0.3)\n",
    "\n",
    "X_train, y_train = X[indice:], y[indice:]\n",
    "X_test, y_test = X[:indice], y[:indice]\n",
    "\n",
    "print('X_train', X_train[2], len(X_train), len(X_train[0]))\n",
    "print('y_train', y_train[2], len(y_train), len(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "frank-prototype",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "# Preparación de los datos \n",
    "def prepare_sequence(seq, to_ix):\n",
    "    # Prepara tensores de indices de palabras a partir de una oración.\n",
    "    # Parámetros:\n",
    "    #   seq: oración\n",
    "    #   to_ix: diccionario de palabras.\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# Entrenar el modelo \n",
    "\n",
    "# Valores antes de entrenar\n",
    "# El elemento i, j de la salida es la puntuación entre la etiqueta j para la palabra i.\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(X_train[0], word_to_ix)\n",
    "    \n",
    "#     print('inputs', inputs, inputs.size())\n",
    "    \n",
    "    tag_scores = model(inputs)\n",
    "    \n",
    "#     print(X_train[0])\n",
    "    \n",
    "    # Clasificación    \n",
    "#     print(tag_scores)\n",
    "\n",
    "# Corridas o épocas\n",
    "for epoch in range(100):  \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch)\n",
    "    \n",
    "    for sentence, etiqueta in zip(X_train, y_train):\n",
    "#         print('sentence', sentence)\n",
    "#         print('tags', etiqueta)\n",
    "        ## Paso 1. Pytorch acumula los gradientes.\n",
    "        # Es necesario limpiarlos\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Paso 2. Se preparan las entradas, es decir, se convierten a\n",
    "        # tensores de índices de palabras.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(etiqueta, etiquetas_a_indice)\n",
    "#         print('targets', targets)\n",
    "        \n",
    "        # Paso 3. Se genera la predicción (forward pass).\n",
    "        tag_scores = model(sentence_in)\n",
    "#         print('tag_scores', tag_scores)\n",
    "\n",
    "        # Paso 4. se calcula la pérdida, los gradientes, y se actualizan los \n",
    "        # parámetros por medio del optimizador.\n",
    "#         print('tag_scores', tag_scores, tag_scores.size())\n",
    "#         print('targets', targets)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "#         print('lossmljsmaLKMSALKmaslkMLKSAMlkmaldksmlkads\\nlksadmldksamlkdsmlakds', loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "optimum-participation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados luego del entrenamiento para la primera frase\n",
      "tensor([[-6.5440e-03, -8.4883e+00, -9.4271e+00, -5.8733e+00, -9.0198e+00,\n",
      "         -8.9164e+00, -1.0297e+01, -8.8808e+00, -6.5064e+00, -1.1495e+01,\n",
      "         -1.0145e+01, -9.5823e+00, -1.0027e+01, -1.0672e+01, -1.0074e+01,\n",
      "         -9.8730e+00, -6.7078e+00],\n",
      "        [-2.7418e-05, -1.6027e+01, -1.5215e+01, -1.1259e+01, -1.4111e+01,\n",
      "         -1.3752e+01, -1.6468e+01, -1.1862e+01, -1.3712e+01, -1.4868e+01,\n",
      "         -1.6624e+01, -1.4648e+01, -1.5587e+01, -1.6179e+01, -1.7498e+01,\n",
      "         -1.2817e+01, -1.5813e+01],\n",
      "        [-2.9053e+00, -1.5297e+00, -2.0516e+00, -8.3235e-01, -5.8421e+00,\n",
      "         -4.4684e+00, -6.7194e+00, -3.5765e+00, -2.2183e+00, -6.7299e+00,\n",
      "         -6.7551e+00, -5.2448e+00, -7.2497e+00, -7.8540e+00, -7.6201e+00,\n",
      "         -6.5781e+00, -6.1395e+00],\n",
      "        [-4.1559e+00, -8.4773e+00, -4.3253e+00, -5.3373e+00, -6.1096e+00,\n",
      "         -7.0858e+00, -7.4938e+00, -5.2391e+00, -3.0184e+00, -3.6730e+00,\n",
      "         -6.0236e+00, -3.7607e-01, -4.7057e+00, -4.7690e+00, -6.5093e+00,\n",
      "         -3.0922e+00, -2.0442e+00],\n",
      "        [-1.2019e-03, -1.4220e+01, -1.6042e+01, -1.0909e+01, -1.1546e+01,\n",
      "         -1.2063e+01, -1.3532e+01, -9.1388e+00, -1.3037e+01, -8.6039e+00,\n",
      "         -1.2661e+01, -8.9750e+00, -1.0812e+01, -1.0971e+01, -1.3529e+01,\n",
      "         -7.3928e+00, -9.3352e+00],\n",
      "        [-2.3853e-02, -5.0091e+00, -1.1566e+01, -5.0047e+00, -8.9793e+00,\n",
      "         -7.5317e+00, -1.0274e+01, -4.8587e+00, -1.0064e+01, -7.3147e+00,\n",
      "         -1.0410e+01, -9.4259e+00, -9.9424e+00, -1.0387e+01, -1.1809e+01,\n",
      "         -7.2016e+00, -9.5680e+00],\n",
      "        [-2.1491e-04, -1.2707e+01, -1.3642e+01, -9.2089e+00, -1.2670e+01,\n",
      "         -1.1608e+01, -1.4390e+01, -9.7916e+00, -1.2578e+01, -1.1835e+01,\n",
      "         -1.4200e+01, -1.2081e+01, -1.3802e+01, -1.4373e+01, -1.5403e+01,\n",
      "         -1.0721e+01, -1.3881e+01],\n",
      "        [-6.0914e-05, -1.2673e+01, -1.7212e+01, -1.0399e+01, -1.4231e+01,\n",
      "         -1.2949e+01, -1.5992e+01, -1.0828e+01, -1.5586e+01, -1.4073e+01,\n",
      "         -1.6263e+01, -1.5653e+01, -1.5865e+01, -1.6493e+01, -1.7376e+01,\n",
      "         -1.2848e+01, -1.6121e+01],\n",
      "        [-1.5020e-05, -1.6098e+01, -1.6435e+01, -1.1536e+01, -1.5110e+01,\n",
      "         -1.4242e+01, -1.7238e+01, -1.2851e+01, -1.4570e+01, -1.6153e+01,\n",
      "         -1.7521e+01, -1.6400e+01, -1.6889e+01, -1.7595e+01, -1.8535e+01,\n",
      "         -1.4279e+01, -1.6702e+01],\n",
      "        [-7.3909e-06, -1.6865e+01, -1.7524e+01, -1.2122e+01, -1.5709e+01,\n",
      "         -1.4936e+01, -1.7672e+01, -1.4335e+01, -1.4582e+01, -1.7576e+01,\n",
      "         -1.7940e+01, -1.7539e+01, -1.7462e+01, -1.8239e+01, -1.8715e+01,\n",
      "         -1.5546e+01, -1.5924e+01],\n",
      "        [-2.6085e-03, -8.5072e+00, -1.1923e+01, -7.3909e+00, -9.8224e+00,\n",
      "         -1.0347e+01, -1.2701e+01, -6.4264e+00, -1.1683e+01, -1.2568e+01,\n",
      "         -1.3793e+01, -1.2966e+01, -1.2030e+01, -1.2443e+01, -1.3768e+01,\n",
      "         -1.0002e+01, -1.4330e+01],\n",
      "        [-1.6938e-04, -1.3509e+01, -1.2645e+01, -9.4268e+00, -1.2560e+01,\n",
      "         -1.2187e+01, -1.5066e+01, -9.7454e+00, -1.2315e+01, -1.3765e+01,\n",
      "         -1.5488e+01, -1.3466e+01, -1.4442e+01, -1.5018e+01, -1.6293e+01,\n",
      "         -1.1674e+01, -1.5910e+01],\n",
      "        [-5.2856e-03, -7.0343e+00, -1.3644e+01, -6.9447e+00, -1.0274e+01,\n",
      "         -9.6852e+00, -1.2596e+01, -5.7438e+00, -1.2734e+01, -1.0743e+01,\n",
      "         -1.3551e+01, -1.3094e+01, -1.2144e+01, -1.2559e+01, -1.4425e+01,\n",
      "         -9.3771e+00, -1.3794e+01],\n",
      "        [-3.0377e-02, -5.2819e+00, -7.2432e+00, -4.2873e+00, -7.6312e+00,\n",
      "         -7.5068e+00, -9.7118e+00, -4.8035e+00, -7.7402e+00, -9.9506e+00,\n",
      "         -1.0291e+01, -9.1015e+00, -9.6450e+00, -1.0154e+01, -1.0336e+01,\n",
      "         -8.0927e+00, -1.1153e+01],\n",
      "        [-2.2159e-04, -1.3388e+01, -1.1559e+01, -9.3662e+00, -1.1201e+01,\n",
      "         -1.2107e+01, -1.3951e+01, -9.7327e+00, -1.0877e+01, -1.4121e+01,\n",
      "         -1.4304e+01, -1.2226e+01, -1.2859e+01, -1.3350e+01, -1.4060e+01,\n",
      "         -1.1141e+01, -1.3741e+01],\n",
      "        [-6.0841e+00, -2.0893e+00, -2.8830e+00, -1.8693e+00, -5.9153e+00,\n",
      "         -5.4422e+00, -6.4921e+00, -5.4475e+00, -7.0762e-01, -7.5034e+00,\n",
      "         -6.2721e+00, -5.1010e+00, -6.3801e+00, -6.8797e+00, -6.5972e+00,\n",
      "         -7.3884e+00, -1.9321e+00],\n",
      "        [-2.0112e+00, -4.5572e+00, -4.8353e+00, -2.3840e+00, -6.7481e+00,\n",
      "         -5.1259e+00, -6.5992e+00, -6.2651e+00, -1.7570e+00, -4.9528e+00,\n",
      "         -5.3306e+00, -3.2011e+00, -6.2961e+00, -6.8871e+00, -6.6019e+00,\n",
      "         -5.7403e+00, -6.6831e-01],\n",
      "        [-4.5182e-02, -7.5079e+00, -7.6169e+00, -4.4772e+00, -7.9516e+00,\n",
      "         -6.9704e+00, -8.7495e+00, -6.8262e+00, -4.8401e+00, -6.7580e+00,\n",
      "         -7.9293e+00, -5.6677e+00, -7.9566e+00, -8.4902e+00, -9.0488e+00,\n",
      "         -6.5393e+00, -4.2522e+00],\n",
      "        [-1.7881e-06, -1.9884e+01, -1.9307e+01, -1.3878e+01, -1.7593e+01,\n",
      "         -1.6419e+01, -1.8747e+01, -1.7679e+01, -1.5309e+01, -1.8654e+01,\n",
      "         -1.8132e+01, -1.7522e+01, -1.8657e+01, -1.9563e+01, -1.8883e+01,\n",
      "         -1.6972e+01, -1.4906e+01]])\n"
     ]
    }
   ],
   "source": [
    "# Despligue de la puntuación luego del entrenamiento ## Se puede echar?\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(X_train[0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "   \n",
    "    print(\"Resultados luego del entrenamiento para la primera frase\")\n",
    "    # Las palabras en una oración se pueden etiquetar de tres formas.\n",
    "    # La primera oración tiene 4 palabras \"El perro come manzana\"\n",
    "    # por eso el tensor de salida tiene 4 elementos. \n",
    "    # Cada elemento es un vector de pesos que indica cuál etiqueta tiene más\n",
    "    # posibilidad de estar asociada a la palabra. Es decir hay que calcular \n",
    "    # la posición del valor máximo\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f3a0eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas\n",
      "{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-org': 3, 'B-nat': 4, 'B-art': 5, 'B-eve': 6, 'B-tim': 7, 'B-per': 8, 'I-geo': 9, 'I-gpe': 10, 'I-org': 11, 'I-nat': 12, 'I-art': 13, 'I-eve': 14, 'I-tim': 15, 'I-per': 16}\n",
      "y_test ['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probar el modelo\n",
    "\n",
    "def test_examples(test_data):\n",
    "\n",
    "   with torch.no_grad():\n",
    "      inputs = prepare_sequence(test_data, word_to_ix)\n",
    "      tag_scores = model(inputs)\n",
    "    \n",
    " \n",
    "#    print(\"FRASE\") \n",
    "#    print(\"La frase original\", test_data)    \n",
    "#    print(\"La frase original preprocesada\", inputs)\n",
    "#    print(\"Salida del modelo\", tag_scores)\n",
    "#    print(\"Valores máximos e índices\", max_values(tag_scores))\n",
    "   return max_values(tag_scores)\n",
    "    \n",
    "\n",
    "#print(\"Índice de palabras\")\n",
    "#print(\"word_to_idx\", word_to_ix)\n",
    "\n",
    "print(\"Etiquetas\")\n",
    "print(etiquetas_a_indice)\n",
    "\n",
    "#Frase 1\n",
    "# Las palabras en una oración se pueden etiquetar de tres formas.\n",
    "# La primera oración tiene 3 palabras \"El perro juega\"\n",
    "# por eso el tensor de salida tiene 3 elementos. \n",
    "# Cada elemento es un vector de pesos que indica cuál etiqueta tiene más\n",
    "# posibilidad de estar asociada a la palabra. Es decir hay que calcular \n",
    "# la posición del valor máximo. \n",
    "#   Ejemplo 1: \"El perro juega\" [\"DET\", \"NN\", \"V\"]\n",
    "# Ejemplo: 0, 1, 2 {\"DET\": 0, \"NN\": 1, \"V\": 2} => DET, NN, V \n",
    "test_examples(X_test[0])\n",
    "print('y_test', y_test[0])\n",
    "\n",
    "[0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-generic",
   "metadata": {},
   "source": [
    "4. Evalúe el modelo resultante. Utilice la métrica propuesta por el InternationalWorkshop on Semantic Evaluation (SemEval), una explicación básica está disponible en (Batista, 2018).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d870b4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 9, 0, 0, 0, 0, 0, 0, 3, 0,\n",
      "        0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 8,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7, 11, 11,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([8, 0, 0, 0, 0, 0, 1, 0, 1, 0, 3, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 8, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  1,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 7, 0])\n",
      "tensor([0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        7, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 3, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0])\n",
      "tensor([3, 0, 0, 0, 0, 0])\n",
      "tensor([8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 11,  0,  0,\n",
      "         1,  0,  0,  8, 16,  8,  0,  0,  0,  2,  0,  0,  0,  0,  1,  0,  0,  0,\n",
      "         0,  0,  1,  0])\n",
      "tensor([0, 0, 8, 8, 8, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0,\n",
      "        0])\n",
      "tensor([1, 0, 3, 0, 0, 0, 0, 7, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  8, 16,  0,  1, 16,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  8, 16,  0,  8,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 8,  0,  0,  0,  0,  3,  0,  0,  3,  0,  0,  7, 11,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 0,  0,  0,  2, 11,  0,  0,  0,  0,  2,  2, 11,  0,  0,  0,  0,  0,  0,\n",
      "         1,  0])\n",
      "tensor([ 8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  7, 11, 11, 16, 16,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor([ 8,  0,  0,  0,  8, 16,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 8, 16,  0,  0,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  0,  1,  0,  0,  0,  1,  0,  0, 11,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  3, 11, 16, 16,  0,  0,  0,  0,  0,  0,  0,\n",
      "         7,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 8, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2, 11,  0,  1,  0,  0,\n",
      "         0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  2, 16,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0])\n",
      "tensor([0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 7, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  0,  8, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 7, 7, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([8, 0, 0, 0, 7, 0, 7, 0, 0, 0, 0])\n",
      "tensor([ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7, 11,  0,  0,  0,\n",
      "         0,  0,  2,  0,  0,  0,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 8, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  3, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  1, 15,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  0, 11,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 7, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  8, 16,  0,  1, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0])\n",
      "tensor([0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  1,  9,  0,  0,  0, 11,  0,  0,  0,  0,\n",
      "         0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0])\n",
      "tensor([0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 2,  8, 16, 16,  0,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 11,  0])\n",
      "tensor([ 8, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0])\n",
      "tensor([ 8, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 8, 16,  0,  0,  1,  7,  0,  3,  0,  0,  0,  0,  7,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 7, 7, 0])\n",
      "tensor([0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 2, 16,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  3,  0,  0,  7,  0,  0,\n",
      "         0,  0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  8, 16,  0,  0,  0,  0,  0,  0,  2, 11,\n",
      "         0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 9, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  7, 11, 11,  0,  0,  0,  0,  0,  0,  2,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 2, 0])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0])\n",
      "tensor([ 8, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 0,  0,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  8, 16,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 7, 0, 0, 0, 1, 9,\n",
      "        0])\n",
      "tensor([0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0])\n",
      "tensor([0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 2,  8, 16, 16,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  1,  0,  0])\n",
      "tensor([ 0,  0,  0,  1, 11,  0, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "tensor([ 0,  1, 11,  0,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  1,  8, 16,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 11,\n",
      "        11, 16,  0])\n",
      "tensor([ 8, 16,  0,  0,  0,  0,  0,  3,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "tensor([ 1,  8, 16,  0,  0,  1,  8,  0,  0,  0,  0,  1, 16,  8, 16,  0,  0,  1,\n",
      "         0,  0,  8, 16,  2,  0,  0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  2,  0,  8,  8, 16,  0,  0,  0,  0,  0,  0,  1,\n",
      "         0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "tensor([ 0,  3, 11, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([0, 0, 7, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([0, 0, 8, 0, 0, 0, 0, 0, 7, 0])\n",
      "tensor([0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 8, 16,  0,  0,  0,  0,  0,  0,  0,  1,  0,  2,  0,  0,  3,  0,  0,  0,\n",
      "         0,  0,  0,  1,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  2,  0,  0,  0,  1, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  3, 11,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0, 16,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([8, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  2, 16,  0,  8,  0,  0,  0,  0,  0,  0,  0,  3, 11,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 8, 16,  8, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 3,  0,  1, 11,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([ 8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  1, 11,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 1, 11,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  1, 11,  0,  0,  0,  0,  0,  0,  0,  1,  0,\n",
      "         0])\n",
      "tensor([0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  1, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0, 11,  0,  8,  0,  0,  0,  0,  0,  2,  0,  0,\n",
      "         0,  0])\n",
      "tensor([ 0,  1, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  3, 11,  0,  2,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
      "        0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0,\n",
      "        0, 2, 0, 2, 0, 0, 1, 0, 0, 0])\n",
      "tensor([ 0,  1, 16,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2, 11,  0,  0,  0])\n",
      "tensor([0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 3, 0, 8, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  2, 11,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 7, 0, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,  0,  0,  3, 11,  7,\n",
      "         0,  0,  1, 11, 11,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2, 11,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 7, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7,\n",
      "        0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 7, 9, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 7, 0])\n",
      "tensor([0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 2, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
      "        0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  3, 11, 11, 11,  0,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 7, 0, 0,\n",
      "        0, 0, 1, 9, 0])\n",
      "tensor([ 0,  1, 11,  0,  0,  3,  0,  7,  0,  0,  0,  0,  0])\n",
      "tensor([ 1,  0,  0,  0,  0,  3, 11,  0,  7,  0])\n",
      "tensor([3, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0])\n",
      "tensor([0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 7, 0, 0, 0, 7, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0])\n",
      "tensor([ 3, 11,  0,  0,  1,  0,  0,  8,  0,  0,  0,  0,  1, 11,  0,  1, 16,  0,\n",
      "         0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 8,  0,  0,  0,  0,  0,  0,  3,  7,  0,  0,  0,  0,  0,  0,  0,  0,  2,\n",
      "        16, 16,  0,  0])\n",
      "tensor([0, 0, 0, 3, 0, 0, 0, 7, 0])\n",
      "tensor([ 8,  0,  0,  0,  0,  3,  0,  0,  0,  2,  0,  0,  7,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  1, 11,  0,  0,  1,  0])\n",
      "tensor([ 0,  0,  0,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,\n",
      "         0,  1,  0,  0,  0,  0,  0,  0,  1, 11,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0])\n",
      "tensor([0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "tensor([ 0,  1,  0,  0,  0,  0,  1,  8, 16,  0,  0,  0,  0,  8, 16,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  1,  1, 11,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0, 11,  0])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  7, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0])\n",
      "tensor([ 2,  8, 16, 16,  0,  0,  0,  0,  0,  0,  0,  3,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  7,  0,  8, 16,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 3, 0, 0, 0])\n",
      "tensor([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 8,  0,  0,  8, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2,\n",
      "        0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 3, 0, 2, 0, 0, 1, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0])\n",
      "tensor([1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([8, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 7, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0, 11,  0,  0,  0,\n",
      "         0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 3, 0, 8, 0, 0, 3, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 8, 16,  0,  0,  0,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0, 11,  0])\n",
      "tensor([0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 2, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0])\n",
      "tensor([8, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  2,  9,  0, 11,  0,  7,  0,  0,  0,  0,  0,  0,\n",
      "         1,  0,  0,  0, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0])\n",
      "tensor([8, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 8,  0,  0,  0,  3,  0,  0,  0,  0,  2, 16,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0])\n",
      "tensor([0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0])\n",
      "tensor([ 0,  0,  0,  2, 11,  0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 1,  0,  0,  1, 11,  0,  0,  0,  0,  8,  0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7, 11,  0,  0,  0,  0,  0,\n",
      "         8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0])\n",
      "tensor([0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 7, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 3, 0, 0, 7, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  3, 16,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  9,  0])\n",
      "tensor([0, 0, 0, 1, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2, 11,  9,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  8,  0,  1, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 8, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  1,  0,  0,  2, 11,  0])\n",
      "tensor([0, 0, 3, 0, 0, 0, 0])\n",
      "tensor([0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0])\n",
      "tensor([0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  1, 15,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  1, 11,  0,  7,  0,  0,  0,  0,  0,  7,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 3, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 8, 0, 0, 0, 1, 0, 7, 0, 0, 0, 0])\n",
      "tensor([ 8,  8, 16,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  1,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 0, 3, 0, 0, 7, 0, 0])\n",
      "tensor([ 0,  8, 16,  0])\n",
      "tensor([3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  0, 16,  0,  0,  7,  0,  0])\n",
      "tensor([ 0,  8, 16,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         3,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  1,  0, 16,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 11,  0,  0,  0,  1,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 0,  1, 11, 16, 16,  8,  0,  7,  0,  0,  2, 11,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 0,  0,  0,  0,  1, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  2,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 9, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 7, 0,\n",
      "        0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([ 8, 16,  0,  0,  0,  0,  0,  0,  1,  9,  0,  0,  0,  0, 11,  0,  0,  0,\n",
      "         0,  0,  0,  0,  1,  0])\n",
      "tensor([0, 0, 2, 0, 0, 0, 0, 7, 9, 0, 0])\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 11,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 8, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 1, 0, 0, 0, 7, 0, 0])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 8, 16,  0,  7,  0,  1,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,\n",
      "        11,  0,  0,  0,  2, 11,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 8, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0, 11,  0,  0,  0,  0,  1,  0,  0,  0])\n",
      "sorted_labels ['O', 'B-geo', 'B-gpe', 'B-org', 'B-nat', 'B-art', 'B-eve', 'B-tim', 'B-per', 'I-geo', 'I-gpe', 'I-org', 'I-nat', 'I-art', 'I-eve', 'I-tim', 'I-per'] 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/walter/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=['O', 'B-geo', 'B-gpe', 'B-org', 'B-nat', 'B-art', 'B-eve', 'B-tim', 'B-per', 'I-geo', 'I-gpe', 'I-org', 'I-nat', 'I-art', 'I-eve', 'I-tim', 'I-per'] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-247-15d18f556d3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m          -1.6126e+01, -1.8655e+01]\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn_crfsuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_classification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn_crfsuite/metrics.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(y_true, y_pred, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my_true_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0my_pred_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn_crfsuite/metrics.py\u001b[0m in \u001b[0;36mflat_classification_report\u001b[0;34m(y_true, y_pred, labels, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m                           \"will result in an error\", FutureWarning)\n\u001b[1;32m     73\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1980\u001b[0m     micro_is_accuracy = ((y_type == 'multiclass' or y_type == 'binary') and\n\u001b[1;32m   1981\u001b[0m                          (not labels_given or\n\u001b[0;32m-> 1982\u001b[0;31m                           (set(labels) == set(unique_labels(y_true, y_pred)))))\n\u001b[0m\u001b[1;32m   1983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# Check that we don't mix string type with number type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mix of label input types (string and number)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "# #evaluator = Evaluator(true, pred, tags=['LOC', 'PER'])\n",
    "\n",
    "# # y_pred = model(X_test)\n",
    "# # labels = list(model.classes_)\n",
    "# # labels.remove('O') # remove 'O' label from evaluation\n",
    "# # sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n",
    "# # print(sklearn_crfsuite.metrics.flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))\n",
    "\n",
    "# import sklearn_crfsuite\n",
    "# from sklearn_crfsuite import metrics\n",
    "\n",
    "# def max_values(x):\n",
    "#     # Retorna el valor máximo y en índice o la posición del valor en un vector x.\n",
    "#     # Parámetros: \n",
    "#     #    x: vector con los datos. \n",
    "#     # Salida: \n",
    "#     #    out: valor \n",
    "#     #    inds: índice\n",
    "#     out, inds = torch.max(x,dim=1)   \n",
    "#     return out, inds\n",
    "\n",
    "# y_pred = []\n",
    "\n",
    "# # print('X_test', X_test)\n",
    "\n",
    "# # for i in range(len(X_test)):\n",
    "    \n",
    "# #     with torch.no_grad():\n",
    "        \n",
    "# #         input = prepare_sequence(X_test[i], word_to_ix)\n",
    "        \n",
    "# #         tag_scores = model(inputs)\n",
    "# #         print(tag_scores)\n",
    "# # #         print(tag_scores)\n",
    "\n",
    "# #     _, tag_pred = max_values(tag_scores)\n",
    "# # #     print('tag_pred', tag_pred)\n",
    "# # #         print(tag_pred)\n",
    "# #     y_pred.append(tag_pred)\n",
    "    \n",
    "\n",
    "# for elem in X_test:\n",
    "    \n",
    "# #     with torch.no_grad():\n",
    "        \n",
    "# #         input = prepare_sequence(elem, word_to_ix)\n",
    "        \n",
    "# #         tag_scores = model(inputs)\n",
    "# #         print(tag_scores)\n",
    "# # #         print(tag_scores)\n",
    "\n",
    "# #     _, tag_pred = max_values(tag_scores)\n",
    "# #     print('tag_pred', tag_pred)\n",
    "# #         print(tag_pred)\n",
    "#     _, tag_pred = test_examples(elem)\n",
    "#     print(tag_pred)\n",
    "#     y_pred.append(tag_pred)\n",
    "        \n",
    "# sorted_labels = [k for k, v in etiquetas_a_indice.items()]\n",
    "# print('sorted_labels', sorted_labels, len(sorted_labels))\n",
    "# # print('y_test', y_test[0], len(y_test[0]))\n",
    "# # print('y_pred', y_pred[0], len(y_pred[0]))\n",
    "\n",
    "# # [0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n",
    "# # ['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n",
    "\n",
    "# [-4.7684e-07, -2.0116e+01, -2.0555e+01, -1.4824e+01, -1.8221e+01,\n",
    "#          -1.6978e+01, -1.9745e+01, -1.6455e+01, -1.8519e+01, -1.7782e+01,\n",
    "#          -1.9167e+01, -1.7576e+01, -1.9413e+01, -2.0180e+01, -2.0123e+01,\n",
    "#          -1.6126e+01, -1.8655e+01]\n",
    "\n",
    "# print(sklearn_crfsuite.metrics.flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "5929721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'B-geo', 'B-gpe', 'B-org', 'B-nat', 'B-art', 'B-eve', 'B-tim', 'B-per', 'I-geo', 'I-gpe', 'I-org', 'I-nat', 'I-art', 'I-eve', 'I-tim', 'I-per'] sorted_labels\n",
      "['B-geo', 'B-gpe', 'B-org', 'B-nat', 'B-art', 'B-eve', 'B-tim', 'B-per', 'I-geo', 'I-gpe', 'I-org', 'I-nat', 'I-art', 'I-eve', 'I-tim', 'I-per'] labels_procesed\n",
      "y_test [['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O'], ['B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O', 'O']] 300\n",
      "---------------\n",
      "pred_labels [['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O'], ['B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org', 'I-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'B-org', 'O', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O']] 300\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-geo      0.443     0.567     0.497       164\n",
      "       B-gpe      0.425     0.420     0.422        81\n",
      "       B-org      0.202     0.138     0.164       130\n",
      "       B-nat      0.000     0.000     0.000         1\n",
      "       B-art      0.000     0.000     0.000         7\n",
      "       B-eve      0.000     0.000     0.000         0\n",
      "       B-tim      0.434     0.392     0.412       125\n",
      "       B-per      0.385     0.340     0.361       103\n",
      "       I-geo      0.467     0.159     0.237        44\n",
      "       I-gpe      0.000     0.000     0.000         1\n",
      "       I-org      0.288     0.221     0.250        95\n",
      "       I-nat      0.000     0.000     0.000         0\n",
      "       I-art      0.000     0.000     0.000         6\n",
      "       I-eve      0.000     0.000     0.000         0\n",
      "       I-tim      0.500     0.022     0.043        45\n",
      "       I-per      0.485     0.311     0.379       103\n",
      "\n",
      "   micro avg      0.392     0.320     0.353       905\n",
      "   macro avg      0.227     0.161     0.173       905\n",
      "weighted avg      0.384     0.320     0.332       905\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/walter/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=['B-geo', 'B-gpe', 'B-org', 'B-nat', 'B-art', 'B-eve', 'B-tim', 'B-per', 'I-geo', 'I-gpe', 'I-org', 'I-nat', 'I-art', 'I-eve', 'I-tim', 'I-per'] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "/home/walter/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/walter/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/walter/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/walter/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/walter/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/walter/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy \n",
    "\n",
    "# Probar el modelo\n",
    "\n",
    "# Funciones utilitarias\n",
    "\n",
    "def max_values(x):\n",
    "    # Retorna el valor máximo y en índice o la posición del valor en un vector x.\n",
    "    # Parámetros: \n",
    "    #    x: vector con los datos. \n",
    "    # Salida: \n",
    "    #    out: valor \n",
    "    #    inds: índice\n",
    "    out, inds = torch.max(x,dim=1)   \n",
    "    return out, inds\n",
    "\n",
    "def test_examples(test_data):\n",
    "\n",
    "   with torch.no_grad():\n",
    "      inputs = prepare_sequence(test_data, word_to_ix)\n",
    "      tag_scores = model(inputs)\n",
    "        \n",
    "      _, pred = max_values(tag_scores)\n",
    "    \n",
    "#    print(\"Salida del modelo\", tag_scores)\n",
    "#    print(\"Valores máximos e índices\", max_values(tag_scores))    \n",
    "    \n",
    "   return pred.tolist()\n",
    "#    print(\"La frase original\", test_data)    \n",
    "#    print(\"La frase original preprocesada\", inputs)\n",
    "    \n",
    "\n",
    "#print(\"Índice de palabras\")\n",
    "#print(\"word_to_idx\", word_to_ix)\n",
    "\n",
    "print(\"Etiquetas\")\n",
    "# print(etiquetas_a_indice)\n",
    "\n",
    "#Frase 1\n",
    "# Las palabras en una oración se pueden etiquetar de tres formas.\n",
    "# La primera oración tiene 3 palabras \"El perro juega\"\n",
    "# por eso el tensor de salida tiene 3 elementos. \n",
    "# Cada elemento es un vector de pesos que indica cuál etiqueta tiene más\n",
    "# posibilidad de estar asociada a la palabra. Es decir hay que calcular \n",
    "# la posición del valor máximo. \n",
    "#   Ejemplo 1: \"El perro juega\" [\"DET\", \"NN\", \"V\"]\n",
    "# Ejemplo: 0, 1, 2 {\"DET\": 0, \"NN\": 1, \"V\": 2} => DET, NN, V \n",
    "y_pred = []\n",
    "y_test_procesed = []\n",
    "\n",
    "print(y_test[0])\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    y_pred.append(test_examples(X_test[i]))\n",
    "    y_test_procesed.append(prepare_sequence(y_test[i], etiquetas_a_indice).tolist())\n",
    "    \n",
    "# print(y_test[0])\n",
    "# print('-------')\n",
    "# test_examples(X_test[1])\n",
    "# print(y_test[1])\n",
    "# print('-------')\n",
    "# test_examples(X_test[2])\n",
    "# print(y_test[2])\n",
    "# print('-------')\n",
    "# test_examples(X_test[3])\n",
    "# print(y_test[3])\n",
    "# print('-------')\n",
    "# test_examples(X_test[4])\n",
    "# print(y_test[4])\n",
    "\n",
    "# [-3.1003e+00, -9.2929e+00, -3.9115e-01, -3.6556e+00, -7.8349e+00,\n",
    "#          -6.5042e+00, -8.1129e+00, -1.1431e+01, -1.5858e+00, -5.1965e+00,\n",
    "#          -7.3174e+00, -6.6276e+00, -7.0125e+00, -7.3155e+00, -7.7962e+00,\n",
    "#          -8.8214e+00, -3.3208e+00]\n",
    "\n",
    "# [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n",
    "\n",
    "sorted_labels = [k for k, v in etiquetas_a_indice.items()]\n",
    "print(sorted_labels, 'sorted_labels')\n",
    "labels_procesed = deepcopy(sorted_labels[1:])\n",
    "print(labels_procesed, 'labels_procesed')\n",
    "\n",
    "pred_labels = []\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    temp = []\n",
    "    for elem in y_pred[i]:\n",
    "        temp.append(sorted_labels[elem])\n",
    "    pred_labels.append(temp[:])\n",
    "\n",
    "# for elem in y_pred[index]:\n",
    "#     pred_labels.append(sorted_labels[elem])\n",
    "# print('sorted_labels', sorted_labels, len(sorted_labels))\n",
    "# print('---------------')\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if len(y_test[i]) != len(pred_labels[i]):\n",
    "        print('_________________________________', i)\n",
    "        print('y_test[i]', y_test[i], len(y_test[i]))\n",
    "        print('pred_labels[i]', pred_labels[i], len(pred_labels[i]))\n",
    "\n",
    "print('y_test', y_test[:3], len(y_test))\n",
    "\n",
    "print('---------------')\n",
    "print('pred_labels', pred_labels[:3], len(pred_labels))\n",
    "\n",
    "print(sklearn_crfsuite.metrics.flat_classification_report(y_test, pred_labels, labels=labels_procesed, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "true = collect_named_entities(y_test[index])\n",
    "\n",
    "    \n",
    "# print(pred_labels)\n",
    "    \n",
    "pred = collect_named_entities(pred_labels[index])\n",
    "\n",
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a45bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(true, pred, ['geo', 'gpe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0abea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_results = {'correct': 0, 'incorrect': 0, 'partial': 0,\n",
    "                   'missed': 0, 'spurious': 0, 'possible': 0, 'actual': 0, 'precision': 0, 'recall': 0}\n",
    "\n",
    "# overall results\n",
    "results = {'strict': deepcopy(metrics_results),\n",
    "           'ent_type': deepcopy(metrics_results),\n",
    "           'partial':deepcopy(metrics_results),\n",
    "           'exact':deepcopy(metrics_results)\n",
    "          }\n",
    "\n",
    "labels_test = ['nat', 'geo', 'tim', 'eve', 'per', 'org', 'art', 'gpe']\n",
    "\n",
    "# results aggregated by entity type\n",
    "evaluation_agg_entities_type = {e: deepcopy(results) for e in labels_test}\n",
    "\n",
    "for true_ents, pred_ents in zip(y_test, pred_labels):\n",
    "#     print(true_ents)\n",
    "#     print(pred_ents)\n",
    "    \n",
    "    # compute results for one message\n",
    "    tmp_results, tmp_agg_results = compute_metrics(\n",
    "        collect_named_entities(true_ents), collect_named_entities(pred_ents),  labels_test\n",
    "    )\n",
    "    \n",
    "    #print(tmp_results)\n",
    "\n",
    "    # aggregate overall results\n",
    "    for eval_schema in results.keys():\n",
    "        for metric in metrics_results.keys():\n",
    "            results[eval_schema][metric] += tmp_results[eval_schema][metric]\n",
    "            \n",
    "    # Calculate global precision and recall\n",
    "        \n",
    "#     print('results', results)\n",
    "        \n",
    "    results = compute_precision_recall_wrapper(results)\n",
    "\n",
    "#     print('results', results)\n",
    "\n",
    "    # aggregate results by entity type\n",
    " \n",
    "    for e_type in labels_test:\n",
    "\n",
    "        for eval_schema in tmp_agg_results[e_type]:\n",
    "#             print(eval_schema)\n",
    "\n",
    "            for metric in tmp_agg_results[e_type][eval_schema]:\n",
    "#                 print(metric)\n",
    "                if metric == 'f1': # if el resultado va a ser 0 continue\n",
    "                    continue\n",
    "                evaluation_agg_entities_type[e_type][eval_schema][metric] += tmp_agg_results[e_type][eval_schema][metric]\n",
    "                \n",
    "        # Calculate precision recall at the individual entity level\n",
    "                \n",
    "        evaluation_agg_entities_type[e_type] = compute_precision_recall_wrapper(evaluation_agg_entities_type[e_type])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c11e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a095fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_agg_entities_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316950d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(y_test, pred_labels, labels_test)\n",
    "results, results_agg = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7613ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d32fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817eac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sn\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# print(\"Confusion Matrix : \")\n",
    "# cm = confusion_matrix(pred_labels, y_test)\n",
    "\n",
    "# print(cm)\n",
    "\n",
    "# df_cm = pd.DataFrame(cm, index = [i for i in \"12345\"],\n",
    "#                   columns = [i for i in \"12345\"])\n",
    "# plt.figure(figsize = (10,7))\n",
    "# sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-court",
   "metadata": {},
   "source": [
    "5. Genere   y   documente   sus   conclusiones   (incluya   al   menos   cuatro conclusiones importantes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a5856",
   "metadata": {},
   "source": [
    "Importancia de limpiar los textos, si no se hubieran eliminado las comillas, signos de puntuación, los textos no servirían\n",
    "\n",
    "Importancia de nuestro modelo en el procesamiento del lenguaje. Es importante para las máquinas comprender nuestra semántica para entendernos a nosotros.\n",
    "\n",
    "Un trabajo que manualmente sería complicadísimo, nuestro modelo es capaz de realizarlo con una precisión aceptable.\n",
    "\n",
    "http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-vietnamese",
   "metadata": {},
   "source": [
    "Referencias\n",
    "----------------\n",
    "*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
