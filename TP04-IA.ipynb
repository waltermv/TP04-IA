{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "modern-calendar",
   "metadata": {},
   "source": [
    "### Instituto Tecnologico de Costa Rica (ITCR)\n",
    "### Sede Interuniversitaria de Alajuela\n",
    "### Escuela de Computacion\n",
    "### Curso: Inteligencia Artificial\n",
    "### Estudiantes: \n",
    "\n",
    " - Brandon Ledezma Fernández - 2018185574\n",
    " - Walter Morales Vásquez - 2018212846\n",
    "\n",
    "### Profesora:\n",
    " \n",
    " - Maria Auxiliadora Mora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-soccer",
   "metadata": {},
   "source": [
    "# Tarea Programada Número 4\n",
    "---\n",
    "#### Introducción:\n",
    "En este trabajo práctico se aplicarán conceptos básicos de aprendizaje automático\n",
    "utilizando redes neuronales recurrentes para resolver problemas que involucran el\n",
    "procesamiento de lenguaje natural.\n",
    "\n",
    "El o los estudiantes deberán realizar dos ejercicios. El primero consiste en implementar\n",
    "una red neuronal recurrente aplicada a un problema de clasificación de textos de opinión\n",
    "sobre prendas de vestir de mujer. El segundo ejercicio consiste en reconocer nombres de\n",
    "entidades en textos (NER, Named-Entity Recognition)\n",
    "\n",
    "El objetivo del trabajo es poner en práctica las habilidades de investigación y el\n",
    "conocimiento adquirido durante el curso sobre redes neuronales por medio de\n",
    "ejercicios prácticos que permitan al estudiante experimentar con el aprendizaje profundo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "unavailable-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ner_evaluation.ner_eval import Evaluator\n",
    "from ner_evaluation.ner_eval import collect_named_entities\n",
    "from ner_evaluation.ner_eval import compute_metrics\n",
    "from ner_evaluation.ner_eval import compute_precision_recall_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-section",
   "metadata": {},
   "source": [
    "## A. Clasificación   de   textos   con   redes   neuronales   recurrentes  LSTM utilizando Pytorch.\n",
    "\n",
    "Se desea que, dado un comentario de revisión de una prenda de vestir, predecir la calificación dada por el comprador. La calificación toma valores enteros entre 1 y 5, donde 1 corresponde a la peor calificación y 5 a la mejor.\n",
    "\n",
    "Datos: Utilice los datos de evaluación de prendas de vestir de mujer disponibles en Kaggle (nicapotato, 2018) para:\n",
    "\n",
    "1. Cargue y prepare los datos para ser introducidos a la red LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "surrounded-badge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23486.000000</td>\n",
       "      <td>23486.000000</td>\n",
       "      <td>23486.000000</td>\n",
       "      <td>23486.000000</td>\n",
       "      <td>23486.000000</td>\n",
       "      <td>23486.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11742.500000</td>\n",
       "      <td>918.118709</td>\n",
       "      <td>43.198544</td>\n",
       "      <td>4.196032</td>\n",
       "      <td>0.822362</td>\n",
       "      <td>2.535936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6779.968547</td>\n",
       "      <td>203.298980</td>\n",
       "      <td>12.279544</td>\n",
       "      <td>1.110031</td>\n",
       "      <td>0.382216</td>\n",
       "      <td>5.702202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5871.250000</td>\n",
       "      <td>861.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11742.500000</td>\n",
       "      <td>936.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>17613.750000</td>\n",
       "      <td>1078.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>23485.000000</td>\n",
       "      <td>1205.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>122.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0   Clothing ID           Age        Rating  \\\n",
       "count  23486.000000  23486.000000  23486.000000  23486.000000   \n",
       "mean   11742.500000    918.118709     43.198544      4.196032   \n",
       "std     6779.968547    203.298980     12.279544      1.110031   \n",
       "min        0.000000      0.000000     18.000000      1.000000   \n",
       "25%     5871.250000    861.000000     34.000000      4.000000   \n",
       "50%    11742.500000    936.000000     41.000000      5.000000   \n",
       "75%    17613.750000   1078.000000     52.000000      5.000000   \n",
       "max    23485.000000   1205.000000     99.000000      5.000000   \n",
       "\n",
       "       Recommended IND  Positive Feedback Count  \n",
       "count     23486.000000             23486.000000  \n",
       "mean          0.822362                 2.535936  \n",
       "std           0.382216                 5.702202  \n",
       "min           0.000000                 0.000000  \n",
       "25%           1.000000                 0.000000  \n",
       "50%           1.000000                 1.000000  \n",
       "75%           1.000000                 3.000000  \n",
       "max           1.000000               122.000000  "
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nRowsRead = 1000\n",
    "\n",
    "# Se carga el archivo con los datos solicitados (defaultofcredit.csv) y se define\n",
    "# a la columna \"default_payment_next_month\" como la objetivo.\n",
    "reviews = pd.read_csv('./data/Womens Clothing E-Commerce Reviews.csv', delimiter=',')#, nrows=nRowsRead)\n",
    "#reviews = pd.read_csv('./data/a.csv')\n",
    "reviews.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "cd7d8590",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how does\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\" u \": \" you \",\n",
    "\" ur \": \" your \",\n",
    "\" n \": \" and \"}\n",
    "\n",
    "def cont_to_exp(x):\n",
    "    if type(x) is str:\n",
    "        x = x.replace('\\\\', '')\n",
    "        for key in contractions:\n",
    "            value = contractions[key]\n",
    "            x = x.replace(key, value)\n",
    "        return x\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "reviews['Review Text'] = reviews['Review Text'].apply(lambda x: cont_to_exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "6c07d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete missing observations for following variables\n",
    "# for x in [\"Clothing ID\",\"Age\",\"Title\",\"Review Text\",\"Rating\",\"Recommended IND\",\"Positive Feedback Count\",\"Division Name\",\"Department Name\",\"Class Name\"]:\n",
    "#     reviews = reviews[reviews[x].notnull()]\n",
    "    \n",
    "# X = reviews.drop(columns = reviews.columns[3:5])\n",
    "# X = pd.get_dummies(X)\n",
    "# y = reviews['Review Text']\n",
    "# y = [elem.split() for elem in y]\n",
    "\n",
    "#reviews['Rating'] = reviews[(reviews['Rating'] >= 1)  (reviews['Rating'] <= 5)]\n",
    "\n",
    "reviews = reviews[reviews[\"Review Text\"].notnull()]\n",
    "reviews = reviews[reviews[\"Rating\"].notnull()]\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "def preprocesar_texto(texto):\n",
    "    \n",
    "    texto = texto.lower()\n",
    "    texto = \"\".join([ch if ch not in punctuation else \" \" for ch in texto])\n",
    "#     texto = \"\".join([ch for ch in texto if ch not in punctuation])\n",
    "    return texto\n",
    "\n",
    "# reviews['Review Text'] = reviews['Review Text'].apply(lambda x: preprocesar_texto(x))\n",
    "\n",
    "X = reviews['Review Text']#.tolist()\n",
    "X = X.tolist()\n",
    "\n",
    "for i in range(len(X)):\n",
    "    X[i] = preprocesar_texto(X[i])\n",
    "\n",
    "#print(X)\n",
    "\n",
    "X = [elem.split() for elem in X]\n",
    "\n",
    "# for i in range(len(X)):\n",
    "#     temp = []\n",
    "#     for j in range(len(X[i])):\n",
    "#         temp.append(preprocesar_texto(X[i][j]))\n",
    "#     X[i] = temp\n",
    "\n",
    "y = reviews['Rating']#.tolist()\n",
    "\n",
    "word_to_ix = {}\n",
    "\n",
    "\n",
    "\n",
    "for review in X:\n",
    "#     print(review)\n",
    "#     if review not in word_to_ix:\n",
    "#         word_to_ix[review] = len(word_to_ix)\n",
    "    for word in review:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)+1\n",
    "\n",
    "ratings = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "democratic-cable",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_lenght = 0\n",
    "for elem in X:\n",
    "    if max_lenght < len(elem):\n",
    "        max_lenght = len(elem)\n",
    "\n",
    "        \n",
    "# for i in range(len(X)):\n",
    "#     largo = len(X[i])\n",
    "#     if largo < max_lenght:\n",
    "#         X[i] += ['<pad>']*(max_lenght-largo)\n",
    "# print(unique)\n",
    "\n",
    "print(max_lenght)\n",
    "\n",
    "palabras_diferentes = set([elem for row in X for elem in row])\n",
    "cantidad_palabras = len(palabras_diferentes)\n",
    "\n",
    "print(cantidad_palabras)\n",
    "\n",
    "#print(palabras_diferentes)\n",
    "# palabras_diferentes\n",
    "# np.savetxt('output.txt', arreglo, delimiter=',')\n",
    "# X\n",
    "# palabras_diferentes\n",
    "# Preparación de los datos \n",
    "def prepare_sequence(seq, to_ix):\n",
    "    # Prepara tensores de indices de palabras a partir de una oración.\n",
    "    # Parámetros:\n",
    "    #   seq: oración\n",
    "    #   to_ix: diccionario de palabras.\n",
    "#     print(seq)\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    #idxs = to_ix[seq]\n",
    "    return idxs\n",
    "\n",
    "X_procesada = []\n",
    "for i in range(len(X)):\n",
    "    X_procesada.append(prepare_sequence(X[i], word_to_ix))\n",
    "    \n",
    "print(X_procesada)\n",
    "    \n",
    "# def padding_(sentences, seq_len):\n",
    "#     features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "#     for ii, review in enumerate(sentences):\n",
    "#         if len(review) != 0:\n",
    "#             features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "#     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "3e9a41cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 9, 18, 19, 693, 14168, 11, 239, 4, 273, 261, 1172, 16, 109, 4, 583, 248, 1894, 747]\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     8     9    18    19   693 14168    11   239     4\n",
      "   273   261  1172    16   109     4   583   248  1894   747]\n"
     ]
    }
   ],
   "source": [
    "print(X_procesada[-1])\n",
    "\n",
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "X_procesada = padding_(X_procesada, max_lenght)\n",
    "print(X_procesada[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-likelihood",
   "metadata": {},
   "source": [
    "2. Utilizando PyTorch defina una red recurrente LSTM para procesar el conjunto de datos y clasificar los comentarios de usuario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "intimate-number",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor([[1, 1],\n",
      "        [3, 3]])\n",
      "tensor([[2., 2.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def tensor_mean(input):\n",
    "    \n",
    "    #output = torch.tensor(np.zeros(input[0].size(0)))\n",
    "    print('input', input)\n",
    "    #input?\n",
    "    output = input.clone().detach()\n",
    "    #output.grad_fn.data.copy_(input.grad_fn.data)\n",
    "    \n",
    "    input = input[torch.arange(input.size(0))==0].double()\n",
    "#     print('output', output)\n",
    "#     print('input', input)\n",
    "#     print('output', output)\n",
    "#     print('output', output[0])\n",
    "#     print('input[0].size(0)', input[0].size(0))\n",
    "    \n",
    "    for i in range(1, output.size(0)):\n",
    "        for j in range(output[i].size(0)):\n",
    "            input[0][j] += output[i][j]\n",
    "            \n",
    "    for i in range(input[0].size(0)):\n",
    "        input[0][i] /= output.size(0)\n",
    "    return input\n",
    "    #return torch.tensor([output.tolist()], grad_fn=input.grad_fn)#requires_grad=False) grad_fn=input.grad\n",
    "\n",
    "# Funciones utilitarias\n",
    "\n",
    "def max_values(x):\n",
    "    # Retorna el valor máximo y en índice o la posición del valor en un vector x.\n",
    "    # Parámetros: \n",
    "    #    x: vector con los datos. \n",
    "    # Salida: \n",
    "    #    out: valor \n",
    "    #    inds: índice\n",
    "    out, inds = torch.max(x,dim=1)   \n",
    "    return out, inds\n",
    "\n",
    "print(tensor_mean(torch.tensor([[1,1],[3,3]])))\n",
    "#max_values(torch.tensor([[1,2,1,5,9]]))\n",
    "\n",
    "def clone_vs_deepcopy(x):\n",
    "    import copy\n",
    "    import torch\n",
    "\n",
    "    x_clone = x.clone()\n",
    "    x_deep_copy = copy.deepcopy(x)\n",
    "    #\n",
    "    x.mul_(-1)\n",
    "    print(f'x = {x}')\n",
    "    print(f'x_clone = {x_clone}')\n",
    "    print(f'x_deep_copy = {x_deep_copy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "approximate-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo\n",
    "\n",
    "# El modelo es una clase que debe heredar de nn.Module\n",
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    # Incialización del modelo\n",
    "#     def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "#         super(LSTMTagger, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "\n",
    "#         # Primero se pasa la entrada a través de una capa Embedding. \n",
    "#         # Esta capa construye una representación de los tokens de \n",
    "#         # un texto donde las palabras que tienen el mismo significado \n",
    "#         # tienen una representación similar.\n",
    "        \n",
    "#         # Esta capa captura mejor el contexto y son espacialmente \n",
    "#         # más eficientes que las representaciones vectoriales (one-hot vector).\n",
    "#         # En Pytorch, se usa el módulo nn.Embedding para crear esta capa, \n",
    "#         # que toma el tamaño del vocabulario y la longitud deseada del vector \n",
    "#         # de palabras como entrada. \n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "#         # El LSTM toma word_embeddings como entrada y genera estados ocultos\n",
    "#         # con dimensionalidad hidden_dim.  \n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "#         # La capa lineal mapea el espacio de estado oculto \n",
    "#         # al espacio de etiquetas\n",
    "#         self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "        #print(embeds.shape)  #[50, 500, 1000]\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "#     def forward(self, sentence):\n",
    "        \n",
    "#         # Pase hacia adelante de la red. \n",
    "#         # Parámetros:\n",
    "#         #    sentence: la oración a procesar\n",
    "#         embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "#         print('embeds', embeds)\n",
    "#         lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        \n",
    "#         print('lstm_out', lstm_out)\n",
    "#         tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "#         print('tag_space', tag_space)\n",
    "        \n",
    "#         # Se utiliza softmax para devolver un peso por etiqueta\n",
    "#         tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        \n",
    "#         return tag_scores#torch.mean(tag_scores,0)\n",
    "\n",
    "# Instanciación del modelo, definición de la función de pérdida y del optimizador   \n",
    "\n",
    "# Hiperparámetros de la red\n",
    "# Valores generalmente altos (32 o 64 dimensiones).\n",
    "# Se definen pequeños, para ver cómo cambian los pesos durante el entrenamiento.\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "# Instancia del modelo\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(ratings)) # len([1,2,3,4,5]))\n",
    "\n",
    "# Función de pérdida: Negative Log Likelihood Loss (NLLL). \n",
    "# Útil para problemas de clasificacion con C clases.\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# Optimizador Stochastic Gradient Descent  \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "a1ccf511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo\n",
    "\n",
    "# El modelo es una clase que debe heredar de nn.Module\n",
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.5):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.n_hidden = n_hidden   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        sigmoid_last = sigmoid_out[:, -1]               # (batch_size, 1)\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h\n",
    "\n",
    "# Instanciación del modelo, definición de la función de pérdida y del optimizador   \n",
    "\n",
    "# Hiperparámetros de la red\n",
    "# Valores generalmente altos (32 o 64 dimensiones).\n",
    "# Se definen pequeños, para ver cómo cambian los pesos durante el entrenamiento.\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 256\n",
    "n_layers = 2\n",
    "\n",
    "# Instancia del modelo\n",
    "# model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(ratings), n_layers) # len([1,2,3,4,5]))\n",
    "\n",
    "model = LSTMTagger(len(word_to_ix), EMBEDDING_DIM, HIDDEN_DIM, len(ratings), n_layers)\n",
    "\n",
    "# Función de pérdida: Negative Log Likelihood Loss (NLLL). \n",
    "# Útil para problemas de clasificacion con C clases.\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# Optimizador Stochastic Gradient Descent  \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-kentucky",
   "metadata": {},
   "source": [
    "3. Separe las muestras en datos de entrenamiento y evaluación y entrene el modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "traditional-excitement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 2 3 4 5 4 6] [4]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        X_procesada,\n",
    "                                        y.values.reshape(-1,1),\n",
    "                                        train_size   = 0.7,\n",
    "                                        random_state = 1234,\n",
    "                                        shuffle      = False\n",
    "                                    )\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(X_train[:10]), torch.from_numpy(y_train[:10]))\n",
    "valid_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# print('X_train', X_train.values)\n",
    "# print('X_test', X_test.values)\n",
    "# print('y_train', y_train)\n",
    "# print('y_test', y_test)\n",
    "\n",
    "#print('X.values.tolist()', X.values.tolist())\n",
    "\n",
    "#test = torch.tensor(y.values.tolist())\n",
    "\n",
    "# X_train = torch.tensor(X_train.values.astype(np.float32)) \n",
    "# X_test = torch.tensor(X_test.values.astype(np.float32)) \n",
    "\n",
    "# y_train = torch.tensor(y_train.values.astype(np.float32))\n",
    "# y_test = torch.tensor(y_test.values.astype(np.float32))\n",
    "\n",
    "# train_target = torch.tensor(train['Target'].values.astype(np.float32))\n",
    "# train = torch.tensor(train.drop('Target', axis = 1).values.astype(np.float32)) \n",
    "# train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "# train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "print(X_train[0], y_train[0])\n",
    "# word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "ceramic-words",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag_scores tensor([0.8365, 0.3721, 0.5685, 0.3997, 1.0000, 0.7763, 0.5426, 0.7845, 0.3214,\n",
      "        1.0000, 0.7701, 0.3433, 0.4892, 0.4260, 1.0000, 0.8316, 0.4987, 0.6515,\n",
      "        0.3731, 1.0000, 0.8818, 0.3456, 0.7789, 0.7538, 1.0000, 0.7628, 0.5325,\n",
      "        0.6592, 0.5316, 1.0000, 0.8184, 0.2487, 0.5905, 0.3939, 1.0000, 0.8094,\n",
      "        0.3125, 0.4275, 0.3767, 1.0000, 0.8727, 0.1941, 0.4755, 0.7001, 1.0000,\n",
      "        0.9184, 0.5467, 0.5564, 0.3371, 1.0000], grad_fn=<SelectBackward>)\n",
      "targets tensor([[4],\n",
      "        [5],\n",
      "        [3],\n",
      "        [5],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [5]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2 or more dimensions (got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-328-9410e50f59da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tag_scores'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2379\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2380\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2381\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected 2 or more dimensions (got {})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2 or more dimensions (got 1)"
     ]
    }
   ],
   "source": [
    "# # Preparación de los datos \n",
    "# def prepare_sequence(seq, to_ix):\n",
    "#     # Prepara tensores de indices de palabras a partir de una oración.\n",
    "#     # Parámetros:\n",
    "#     #   seq: oración\n",
    "#     #   to_ix: diccionario de palabras.\n",
    "#     idxs = [to_ix[w] for w in seq]\n",
    "#     #idxs = to_ix[seq]\n",
    "#     return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# Entrenar el modelo \n",
    "\n",
    "# Valores antes de entrenar\n",
    "# El elemento i, j de la salida es la puntuación entre la etiqueta j para la palabra i.\n",
    "# with torch.no_grad():\n",
    "#     inputs = X_train[0]#prepare_sequence(X_train[0], word_to_ix)\n",
    "#     print('inputs', inputs)\n",
    "#     tag_scores = model(inputs)\n",
    "    \n",
    "#     print(X_train[0])\n",
    "    \n",
    "#     # Clasificación    \n",
    "#     print(tag_scores)\n",
    "\n",
    "# Corridas o épocas\n",
    "for epoch in range(500):  # definir epocas por fuera?       ## [N, D, N], [N, D, N]\n",
    "    for sentence, current_ratings in train_loader: ## [1], [2]\n",
    "#         print('sentence', sentence)\n",
    "#         print('current_ratings', current_ratings)\n",
    "        ## Paso 1. Pytorch acumula los gradientes.\n",
    "        # Es necesario limpiarlos\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Paso 2. Se preparan las entradas, es decir, se convierten a\n",
    "        # tensores de índices de palabras.\n",
    "        sentence_in = sentence#prepare_sequence(sentence, word_to_ix)\n",
    "        #print('current_ratings', current_ratings)\n",
    "        #targets = prepare_sequence(rating, ratings)\n",
    "\n",
    "        # Paso 3. Se genera la predicción (forward pass).\n",
    "        tag_scores, _ = model(sentence_in)\n",
    "#         print('tag_scores', tag_scores)\n",
    "        targets = current_ratings\n",
    "        \n",
    "#         print('aslkdmalksmdslakmdaslkmsadkl')\n",
    "        #clone_vs_deepcopy(tag_scores)\n",
    "\n",
    "        # Paso 4. se calcula la pérdida, los gradientes, y se actualizan los \n",
    "        # parámetros por medio del optimizador.\n",
    "        #loss = loss_function(tag_scores, targets.long()).float()\n",
    "        \n",
    "        print('tag_scores', tag_scores)\n",
    "        print('targets', targets)\n",
    "        loss = loss_function(tag_scores.squeeze(), targets)\n",
    "        print('loss', loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Despligue de la puntuación luego del entrenamiento\n",
    "# with torch.no_grad():\n",
    "#     inputs = prepare_sequence(training_data[0], word_to_ix)\n",
    "#     tag_scores = model(inputs)\n",
    "   \n",
    "#     print(\"Resultados luego del entrenamiento para la primera frase\")\n",
    "#     # Las palabras en una oración se pueden etiquetar de tres formas.\n",
    "#     # La primera oración tiene 4 palabras \"El perro come manzana\"\n",
    "#     # por eso el tensor de salida tiene 4 elementos. \n",
    "#     # Cada elemento es un vector de pesos que indica cuál etiqueta tiene más\n",
    "#     # posibilidad de estar asociada a la palabra. Es decir hay que calcular \n",
    "#     # la posición del valor máximo\n",
    "#     print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "af898359",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-326-e8861331a23f>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-326-e8861331a23f>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    for inputs, labels in [X_train[:10], y_train[:10]]#train_loader:\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "print_every = 10\n",
    "step = 0\n",
    "n_epochs = 1  # validation loss increases from ~ epoch 3 or 4\n",
    "clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in [X_train[:10], y_train[:10]]:#train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # making requires_grad = False for the latest set of h\n",
    "        h = tuple([each.data for each in h])   \n",
    "        \n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs)\n",
    "        \n",
    "#         print('output.squeeze()', output.squeeze())\n",
    "#         print('labels.float()', labels.T[0].float())\n",
    "        \n",
    "        loss = criterion(output.squeeze(), labels.T[0].float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            ######################\n",
    "            ##### VALIDATION #####\n",
    "            ######################\n",
    "            model.eval()\n",
    "            valid_losses = []\n",
    "            v_h = model.init_hidden(batch_size)\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "                v_h = tuple([each.data for each in v_h])\n",
    "                \n",
    "                v_output, v_h = model(v_inputs)\n",
    "                v_loss = criterion(output.squeeze(), labels.T[0].float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-pipeline",
   "metadata": {},
   "source": [
    "4. Evalúe el modelo resultante utilizando una matriz de confusión y métricas extraídas a partir de esta (ie. precisión, exhaustividad y F1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-active",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "finnish-aaron",
   "metadata": {},
   "source": [
    "5. Genere y documente sus conclusiones (incluya al menos cuatro conclusionesimportantes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-investment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "frozen-mission",
   "metadata": {},
   "source": [
    "Referencias\n",
    "----------------\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-modem",
   "metadata": {},
   "source": [
    "## B. Reconocimiento   de   nombres   de   entidades   (NER,   Name   EntityRecognition) con redes neuronales recurrentes utilizando Pytorch.\n",
    "\n",
    "El reconocimiento de nombres de entidades (NER)  es  el proceso  de identificar y categorizar elementos clave (ej. entidades) en el texto. Una entidad puede ser cualquier palabra o secuencia de palabras que se refieren a una persona, animal, sitio o cosa (ej.empresa, región geográfica, objeto). Cada entidad detectada se clasifica en una categoría predeterminada. Normalmente, NER se aborda como un problema de etiquetado de secuencias. Una explicación muy detallada de porqué es importante extraer entidades de los textos se encuentra en (Monge, 2020).\n",
    "\n",
    "Los algoritmos de extracción de entidades pueden únicamente detectar la presencia de una entidad y marcarla como tal o pueden detectar y clasificar cada entidad que encuentran.\n",
    "\n",
    "Ejemplo: En una oración como “arbusto de 2 m. flores lila.”.  Cada palabra representa un token donde “arbusto” y “flores” son los elementos de interés a marcar. El etiquetado “token inicial- token interno” es una forma común de indicar dónde comienzan y terminan las entidades. En el ejemplo anterior la etiqueta sería “B O O O B O” donde B representael inicio de la entidad y O cualquier otro token. Para la oración “botones florales rosados.” la etiqueta estaría dada por “B I O” donde “B” marca el token inicio de la entidad e “I” los otros tokens que son parte de esta, es decir “B I” delimita la entidad “botones florales”. \n",
    "\n",
    "Otra forma de marcar y etiquetar, es además de delimitar la entidad, asignar a esta la clase a la que corresponde, por ejemplo: empresa, ciudad, persona, entre otros. Para el presente ejercicio se va a utilizar este enfoque.\n",
    "\n",
    "Utilice los datos para reconocer y clasificar nombres de entidades compartidos en Kaggle por (Ranjan, 2020) para:\n",
    "\n",
    "1. Cargue y prepare los datos para ser introducidos a la red recurrente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "internal-nothing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_ID</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>997</td>\n",
       "      <td>877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sentence: 10718</td>\n",
       "      <td>['Unseeded', 'players', 'advancing', 'included...</td>\n",
       "      <td>['NNP', 'POS', 'NNP', 'NNP', 'VBZ', '.']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sentence_ID                                               Word  \\\n",
       "count              1000                                               1000   \n",
       "unique             1000                                               1000   \n",
       "top     Sentence: 10718  ['Unseeded', 'players', 'advancing', 'included...   \n",
       "freq                  1                                                  1   \n",
       "\n",
       "                                             POS  \\\n",
       "count                                       1000   \n",
       "unique                                       997   \n",
       "top     ['NNP', 'POS', 'NNP', 'NNP', 'VBZ', '.']   \n",
       "freq                                           3   \n",
       "\n",
       "                                                      Tag  \n",
       "count                                                1000  \n",
       "unique                                                877  \n",
       "top     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "freq                                                   13  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nRowsRead = 1000\n",
    "\n",
    "# Se carga el archivo con los datos solicitados (defaultofcredit.csv) y se define\n",
    "# a la columna \"default_payment_next_month\" como la objetivo.\n",
    "ner = pd.read_csv('./data/NER_Dataset.csv', delimiter=',', nrows=nRowsRead)\n",
    "\n",
    "ner.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "corporate-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete missing observations for following variables\n",
    "# for x in [\"Clothing ID\",\"Age\",\"Title\",\"Review Text\",\"Rating\",\"Recommended IND\",\"Positive Feedback Count\",\"Division Name\",\"Department Name\",\"Class Name\"]:\n",
    "#     reviews = reviews[reviews[x].notnull()]\n",
    "    \n",
    "# X = reviews.drop(columns = reviews.columns[3:5])\n",
    "# X = pd.get_dummies(X)\n",
    "# y = reviews['Review Text']\n",
    "# y = [elem.split() for elem in y]\n",
    "\n",
    "#reviews['Rating'] = reviews[(reviews['Rating'] >= 1)  (reviews['Rating'] <= 5)]\n",
    "\n",
    "\n",
    "# ner[\"Word\"] = ner[\"Word\"].str.replace(r\"'\", '', regex=True)\n",
    "# ner[\"Word\"] = ner[\"Word\"].str.replace(r\"[\", '', regex=True)\n",
    "# ner[\"Word\"] = ner[\"Word\"].str.replace(r\"]\", '', regex=True)\n",
    "# ner[\"Word\"] = ner[\"Word\"].str.replace(r\" \", '', regex=True)\n",
    "\n",
    "# ner[\"Word\"] = ner[\"Word\"].str.replace(r\"'\", '', regex=True)\n",
    "# ner[\"Word\"] = ner[\"Word\"].str.replace(r\"[\", '', regex=True)\n",
    "# ner[\"Word\"] = ner[\"Word\"].str.replace(r\"]\", '', regex=True)\n",
    "# ner[\"Word\"] = ner[\"Word\"].str.replace(r\" \", '', regex=True)\n",
    "\n",
    "#ner[\"Word\"] = ner[\"Word\"].Series.str.replace('[', '', regex=True)\n",
    "\n",
    "for x in [\"Word\", \"Tag\"]:\n",
    "#     ner[x] = ner[x].str.replace(\"[\", '', 1, regex=True)\n",
    "#     ner[x] = ner[x][-1].str.replace(\"]\", '', 1, regex=True)\n",
    "    ner[x] = ner[x].str.replace(\"'\", '', regex=True)\n",
    "    ner[x] = ner[x].str.replace(\", \", ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "concerned-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ner['Word']\n",
    "X = [elem[1:-1].split() for elem in X]\n",
    "# X[0] = X[0][1:]\n",
    "# X[-1] = X[0][1:]\n",
    "y = ner['Tag']\n",
    "y = [elem[1:-1].split() for elem in y]\n",
    "# y.pop(0)\n",
    "# y.pop()\n",
    "\n",
    "\n",
    "# def limpiar_datos(dataset):\n",
    "#     for i in range(len(dataset)):\n",
    "#         dataset[i].remove()\n",
    "    \n",
    "word_to_ix = {}\n",
    "for frase in X:\n",
    "#     if review not in word_to_ix:\n",
    "#         word_to_ix[review] = len(word_to_ix)\n",
    "    for palabra in frase:\n",
    "        if palabra not in word_to_ix:\n",
    "            word_to_ix[palabra] = len(word_to_ix)\n",
    "\n",
    "            \n",
    "etiquetas_a_indice = {'O':0,'B-geo':1,'B-gpe':2,'B-org':3,'B-nat':4,'B-art':5,'B-eve':6,'B-tim':7,'B-per':8,\n",
    "                      'I-geo':9,'I-gpe':10,'I-org':11,'I-nat':12,'I-art':13,'I-eve':14,'I-tim':15,'I-per':16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "plain-balloon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-nat', 'B-geo', 'I-tim', 'I-eve', 'I-per', 'I-org', 'B-eve', 'I-gpe', 'O', 'B-org', 'I-geo', 'I-art', 'B-art', 'B-gpe', 'B-per', 'B-tim', 'B-nat'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4783"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "test = flatten(y)\n",
    "# print(test)\n",
    "myset = set(test)\n",
    "print(myset)\n",
    "len(word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-happiness",
   "metadata": {},
   "source": [
    "2. Utilizando PyTorch defina una red recurrente LSTM para procesar, localizar y clasificar las entidades presentes en el texto (como la vista en clase). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "failing-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo\n",
    "\n",
    "# El modelo es una clase que debe heredar de nn.Module\n",
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    # Incialización del modelo\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "\n",
    "        # Primero se pasa la entrada a través de una capa Embedding. \n",
    "        # Esta capa construye una representación de los tokens de \n",
    "        # un texto donde las palabras que tienen el mismo significado \n",
    "        # tienen una representación similar.\n",
    "        \n",
    "        # Esta capa captura mejor el contexto y son espacialmente \n",
    "        # más eficientes que las representaciones vectoriales (one-hot vector).\n",
    "        # En Pytorch, se usa el módulo nn.Embedding para crear esta capa, \n",
    "        # que toma el tamaño del vocabulario y la longitud deseada del vector \n",
    "        # de palabras como entrada. \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # El LSTM toma word_embeddings como entrada y genera estados ocultos\n",
    "        # con dimensionalidad hidden_dim.  \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # La capa lineal mapea el espacio de estado oculto \n",
    "        # al espacio de etiquetas\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # Pase hacia adelante de la red. \n",
    "        # Parámetros:\n",
    "        #    sentence: la oración a procesar\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "\n",
    "#         print('tag_space', tag_space)\n",
    "        \n",
    "        # Se utiliza softmax para devolver un peso por etiqueta\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "#         print('tag_scores', tag_scores)\n",
    "        return tag_scores\n",
    "\n",
    "# Instanciación del modelo, definición de la función de pérdida y del optimizador   \n",
    "\n",
    "# Hiperparámetros de la red\n",
    "# Valores generalmente altos (32 o 64 dimensiones).\n",
    "# Se definen pequeños, para ver cómo cambian los pesos durante el entrenamiento.\n",
    "\n",
    "EMBEDDING_DIM = 6#34\n",
    "HIDDEN_DIM = 6#34\n",
    "\n",
    "# Instancia del modelo\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(etiquetas_a_indice)) # len([1,2,3,4,5]))\n",
    "\n",
    "# Función de pérdida: Negative Log Likelihood Loss (NLLL). \n",
    "# Útil para problemas de clasificacion con C clases.\n",
    "loss_function = nn.NLLLoss()#nn.CrossEntropyLoss()#nn.NLLLoss()\n",
    "\n",
    "# Optimizador Stochastic Gradient Descent  \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-walker",
   "metadata": {},
   "source": [
    "3. Separe las muestras en datos de entrenamiento y evaluación y entrene el modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "purple-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "proporcion = lambda : 0.7\n",
    "\n",
    "# Se randomizan las posiciones de las clases.\n",
    "random.shuffle(X, proporcion)\n",
    "random.shuffle(y, proporcion)\n",
    "\n",
    "indice = int(len(X)*0.3)\n",
    "\n",
    "X_train, y_train = X[indice:], y[indice:]\n",
    "X_test, y_test = X[:indice], y[:indice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "frank-prototype",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "# Preparación de los datos \n",
    "def prepare_sequence(seq, to_ix):\n",
    "    # Prepara tensores de indices de palabras a partir de una oración.\n",
    "    # Parámetros:\n",
    "    #   seq: oración\n",
    "    #   to_ix: diccionario de palabras.\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# Entrenar el modelo \n",
    "\n",
    "# Valores antes de entrenar\n",
    "# El elemento i, j de la salida es la puntuación entre la etiqueta j para la palabra i.\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(X_train[0], word_to_ix)\n",
    "    \n",
    "#     print('inputs', inputs, inputs.size())\n",
    "    \n",
    "    tag_scores = model(inputs)\n",
    "    \n",
    "#     print(X_train[0])\n",
    "    \n",
    "    # Clasificación    \n",
    "#     print(tag_scores)\n",
    "\n",
    "# Corridas o épocas\n",
    "for epoch in range(100):  \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch)\n",
    "    \n",
    "    for sentence, etiqueta in zip(X_train, y_train):\n",
    "#         print('sentence', sentence)\n",
    "#         print('tags', etiqueta)\n",
    "        ## Paso 1. Pytorch acumula los gradientes.\n",
    "        # Es necesario limpiarlos\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Paso 2. Se preparan las entradas, es decir, se convierten a\n",
    "        # tensores de índices de palabras.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(etiqueta, etiquetas_a_indice)\n",
    "#         print('targets', targets)\n",
    "        \n",
    "        # Paso 3. Se genera la predicción (forward pass).\n",
    "        tag_scores = model(sentence_in)\n",
    "#         print('tag_scores', tag_scores)\n",
    "\n",
    "        # Paso 4. se calcula la pérdida, los gradientes, y se actualizan los \n",
    "        # parámetros por medio del optimizador.\n",
    "#         print('tag_scores', tag_scores, tag_scores.size())\n",
    "#         print('targets', targets)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "#         print('lossmljsmaLKMSALKmaslkMLKSAMlkmaldksmlkads\\nlksadmldksamlkdsmlakds', loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "optimum-participation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados luego del entrenamiento para la primera frase\n",
      "tensor([[-6.8141e-04, -1.2355e+01, -9.3799e+00, -9.2961e+00, -1.1362e+01,\n",
      "         -1.0642e+01, -1.2955e+01, -9.4118e+00, -1.0770e+01, -8.6512e+00,\n",
      "         -1.2413e+01, -9.9024e+00, -1.0947e+01, -1.1157e+01, -1.2982e+01,\n",
      "         -9.2915e+00, -1.2291e+01],\n",
      "        [-1.1265e-04, -1.3998e+01, -1.3448e+01, -1.1332e+01, -1.3454e+01,\n",
      "         -1.3149e+01, -1.5578e+01, -9.4931e+00, -1.3690e+01, -1.2628e+01,\n",
      "         -1.5486e+01, -1.3244e+01, -1.4214e+01, -1.3759e+01, -1.6000e+01,\n",
      "         -1.1391e+01, -1.8065e+01],\n",
      "        [-5.8013e+00, -8.5967e-01, -1.5256e+00, -1.3653e+00, -5.4336e+00,\n",
      "         -4.9006e+00, -6.9122e+00, -2.7804e+00, -4.4814e+00, -5.7434e+00,\n",
      "         -6.6125e+00, -6.2157e+00, -6.1050e+00, -6.1386e+00, -7.1772e+00,\n",
      "         -5.8721e+00, -8.4450e+00],\n",
      "        [-5.2604e+00, -6.3049e+00, -5.8139e+00, -3.6296e+00, -6.4044e+00,\n",
      "         -7.1075e+00, -7.3145e+00, -5.8248e+00, -2.8485e+00, -2.6641e+00,\n",
      "         -6.8280e+00, -1.2817e+00, -5.9268e+00, -5.5292e+00, -7.5917e+00,\n",
      "         -3.0812e+00, -6.9701e-01],\n",
      "        [-3.9355e-04, -1.8460e+01, -1.7115e+01, -1.3612e+01, -1.4327e+01,\n",
      "         -1.4582e+01, -1.6023e+01, -1.5543e+01, -1.1937e+01, -1.2500e+01,\n",
      "         -1.5796e+01, -8.1414e+00, -1.3715e+01, -1.3941e+01, -1.6448e+01,\n",
      "         -1.1910e+01, -9.4264e+00],\n",
      "        [-5.7238e-03, -9.6636e+00, -1.0482e+01, -6.3367e+00, -9.6474e+00,\n",
      "         -9.8649e+00, -1.1463e+01, -1.0985e+01, -5.6632e+00, -1.3266e+01,\n",
      "         -1.1951e+01, -8.8513e+00, -1.1079e+01, -1.0832e+01, -1.2312e+01,\n",
      "         -1.2146e+01, -1.0298e+01],\n",
      "        [-1.4340e-04, -1.4287e+01, -1.3769e+01, -1.0302e+01, -1.2538e+01,\n",
      "         -1.2763e+01, -1.4378e+01, -1.1398e+01, -1.0236e+01, -1.2459e+01,\n",
      "         -1.4475e+01, -1.0398e+01, -1.3284e+01, -1.2759e+01, -1.4984e+01,\n",
      "         -1.1296e+01, -1.3098e+01],\n",
      "        [-2.8999e-04, -1.0698e+01, -1.1976e+01, -8.8101e+00, -1.1435e+01,\n",
      "         -1.1251e+01, -1.3543e+01, -1.0432e+01, -1.0351e+01, -1.3563e+01,\n",
      "         -1.3750e+01, -1.1448e+01, -1.2333e+01, -1.2536e+01, -1.4146e+01,\n",
      "         -1.2702e+01, -1.4522e+01],\n",
      "        [-6.9258e-05, -1.4121e+01, -1.4364e+01, -1.0640e+01, -1.3110e+01,\n",
      "         -1.3304e+01, -1.5149e+01, -1.1004e+01, -1.1395e+01, -1.3851e+01,\n",
      "         -1.5385e+01, -1.2318e+01, -1.4292e+01, -1.3642e+01, -1.5839e+01,\n",
      "         -1.2300e+01, -1.5956e+01],\n",
      "        [-5.8913e-03, -6.6020e+00, -7.9278e+00, -6.2508e+00, -8.9332e+00,\n",
      "         -8.3651e+00, -1.1012e+01, -6.5521e+00, -9.3946e+00, -9.9792e+00,\n",
      "         -1.0922e+01, -9.6269e+00, -9.4307e+00, -9.9503e+00, -1.1375e+01,\n",
      "         -9.6584e+00, -1.3161e+01],\n",
      "        [-5.7338e-05, -1.6463e+01, -1.0871e+01, -1.2404e+01, -1.4068e+01,\n",
      "         -1.2660e+01, -1.5571e+01, -1.3683e+01, -1.3348e+01, -1.1089e+01,\n",
      "         -1.4808e+01, -1.2334e+01, -1.3077e+01, -1.3797e+01, -1.5358e+01,\n",
      "         -1.2591e+01, -1.4152e+01],\n",
      "        [-3.4571e-06, -1.8975e+01, -1.5613e+01, -1.3876e+01, -1.6079e+01,\n",
      "         -1.5600e+01, -1.7947e+01, -1.4597e+01, -1.4101e+01, -1.5637e+01,\n",
      "         -1.7907e+01, -1.5239e+01, -1.6828e+01, -1.6251e+01, -1.8363e+01,\n",
      "         -1.4836e+01, -1.8671e+01],\n",
      "        [-7.0562e-03, -7.5952e+00, -9.8597e+00, -5.6280e+00, -8.8400e+00,\n",
      "         -9.1589e+00, -1.0731e+01, -8.9809e+00, -6.1606e+00, -1.1622e+01,\n",
      "         -1.1123e+01, -8.5745e+00, -9.9468e+00, -1.0007e+01, -1.1487e+01,\n",
      "         -1.0906e+01, -1.0191e+01],\n",
      "        [-4.1057e-02, -8.0402e+00, -3.8605e+00, -5.5656e+00, -7.9724e+00,\n",
      "         -6.6803e+00, -9.1340e+00, -9.4282e+00, -6.3146e+00, -5.7060e+00,\n",
      "         -8.3251e+00, -6.1331e+00, -6.6049e+00, -7.9562e+00, -8.8625e+00,\n",
      "         -8.3315e+00, -5.6282e+00],\n",
      "        [-3.4093e-05, -1.7902e+01, -1.1716e+01, -1.2777e+01, -1.4592e+01,\n",
      "         -1.3312e+01, -1.5914e+01, -1.6433e+01, -1.2163e+01, -1.2385e+01,\n",
      "         -1.5274e+01, -1.2241e+01, -1.3555e+01, -1.4388e+01, -1.5768e+01,\n",
      "         -1.4281e+01, -1.2517e+01],\n",
      "        [-1.8699e+00, -1.8108e+00, -8.6436e-01, -1.6308e+00, -5.2707e+00,\n",
      "         -4.2534e+00, -6.6451e+00, -4.9957e+00, -4.1111e+00, -5.3285e+00,\n",
      "         -6.1870e+00, -6.1615e+00, -5.0031e+00, -6.0408e+00, -6.6409e+00,\n",
      "         -7.1744e+00, -6.6717e+00],\n",
      "        [-2.1347e+00, -1.8169e+00, -6.4706e+00, -2.5755e+00, -4.6934e+00,\n",
      "         -5.3873e+00, -6.5255e+00, -1.9306e+00, -4.8652e+00, -3.6038e+00,\n",
      "         -6.3308e+00, -1.1234e+00, -4.6575e+00, -4.9245e+00, -7.0438e+00,\n",
      "         -2.5898e+00, -3.6223e+00],\n",
      "        [-1.2265e-01, -7.3624e+00, -7.2825e+00, -6.2446e+00, -7.3546e+00,\n",
      "         -6.9754e+00, -9.0415e+00, -5.9727e+00, -7.8673e+00, -4.4867e+00,\n",
      "         -8.3491e+00, -2.5916e+00, -6.2708e+00, -7.0765e+00, -9.1184e+00,\n",
      "         -4.5303e+00, -4.8903e+00],\n",
      "        [-3.0994e-06, -2.2036e+01, -1.7746e+01, -1.6732e+01, -1.7438e+01,\n",
      "         -1.6747e+01, -1.9229e+01, -1.7111e+01, -1.6386e+01, -1.4799e+01,\n",
      "         -1.8747e+01, -1.3275e+01, -1.6803e+01, -1.6995e+01, -1.9367e+01,\n",
      "         -1.4595e+01, -1.6081e+01]])\n"
     ]
    }
   ],
   "source": [
    "# Despligue de la puntuación luego del entrenamiento\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(X_train[0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "   \n",
    "    print(\"Resultados luego del entrenamiento para la primera frase\")\n",
    "    # Las palabras en una oración se pueden etiquetar de tres formas.\n",
    "    # La primera oración tiene 4 palabras \"El perro come manzana\"\n",
    "    # por eso el tensor de salida tiene 4 elementos. \n",
    "    # Cada elemento es un vector de pesos que indica cuál etiqueta tiene más\n",
    "    # posibilidad de estar asociada a la palabra. Es decir hay que calcular \n",
    "    # la posición del valor máximo\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-generic",
   "metadata": {},
   "source": [
    "4. Evalúe el modelo resultante. Utilice la métrica propuesta por el InternationalWorkshop on Semantic Evaluation (SemEval), una explicación básica está disponible en (Batista, 2018).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d870b4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted_labels B-geo 17\n",
      "y_test ['B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O'] 300\n",
      "y_pred tensor([ 0,  0,  1, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2, 11,  0,\n",
      "         0]) 300\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [6760, 5700]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-708d30458ed5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m          -1.4595e+01, -1.6081e+01]\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn_crfsuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_classification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn_crfsuite/metrics.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(y_true, y_pred, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my_true_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0my_pred_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn_crfsuite/metrics.py\u001b[0m in \u001b[0;36mflat_classification_report\u001b[0;34m(y_true, y_pred, labels, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m                           \"will result in an error\", FutureWarning)\n\u001b[1;32m     73\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1968\u001b[0m     \"\"\"\n\u001b[1;32m   1969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1970\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    320\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [6760, 5700]"
     ]
    }
   ],
   "source": [
    "#evaluator = Evaluator(true, pred, tags=['LOC', 'PER'])\n",
    "\n",
    "# y_pred = model(X_test)\n",
    "# labels = list(model.classes_)\n",
    "# labels.remove('O') # remove 'O' label from evaluation\n",
    "# sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n",
    "# print(sklearn_crfsuite.metrics.flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "def max_values(x):\n",
    "    # Retorna el valor máximo y en índice o la posición del valor en un vector x.\n",
    "    # Parámetros: \n",
    "    #    x: vector con los datos. \n",
    "    # Salida: \n",
    "    #    out: valor \n",
    "    #    inds: índice\n",
    "    out, inds = torch.max(x,dim=1)   \n",
    "    return out, inds\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "# print('X_test', X_test)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input = prepare_sequence(X_test[i], word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "#         print(tag_scores)\n",
    "\n",
    "        _, tag_pred = max_values(tag_scores)\n",
    "#         print(tag_pred)\n",
    "        y_pred.append(tag_pred)\n",
    "        \n",
    "sorted_labels = [k for k, v in etiquetas_a_indice.items()]\n",
    "print('sorted_labels', sorted_labels[1], len(sorted_labels))\n",
    "print('y_test', y_test[1], len(y_test))\n",
    "print('y_pred', y_pred[1], len(y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(sklearn_crfsuite.metrics.flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5929721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'B-geo', 'B-gpe', 'B-org', 'B-nat', 'B-art', 'B-eve', 'B-tim', 'B-per', 'I-geo', 'I-gpe', 'I-org', 'I-nat', 'I-art', 'I-eve', 'I-tim', 'I-per'] sorted_labels\n",
      "['B-geo', 'B-gpe', 'B-org', 'B-nat', 'B-art', 'B-eve', 'B-tim', 'B-per', 'I-geo', 'I-gpe', 'I-org', 'I-nat', 'I-art', 'I-eve', 'I-tim', 'I-per'] labels_procesed\n",
      "y_test [['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O'], ['B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O', 'O']] 300\n",
      "---------------\n",
      "pred_labels [['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O'], ['B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'B-tim', 'O', 'O', 'O', 'O', 'I-per', 'O', 'B-per', 'O', 'O'], ['O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O', 'B-per', 'O', 'O']] 300\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-geo      0.434     0.543     0.482       164\n",
      "       B-gpe      0.493     0.457     0.474        81\n",
      "       B-org      0.289     0.169     0.214       130\n",
      "       B-nat      0.000     0.000     0.000         1\n",
      "       B-art      0.000     0.000     0.000         7\n",
      "       B-eve      0.000     0.000     0.000         0\n",
      "       B-tim      0.588     0.456     0.514       125\n",
      "       B-per      0.476     0.379     0.422       103\n",
      "       I-geo      0.524     0.250     0.338        44\n",
      "       I-gpe      0.000     0.000     0.000         1\n",
      "       I-org      0.352     0.200     0.255        95\n",
      "       I-nat      0.000     0.000     0.000         0\n",
      "       I-art      0.000     0.000     0.000         6\n",
      "       I-eve      0.000     0.000     0.000         0\n",
      "       I-tim      0.000     0.000     0.000        45\n",
      "       I-per      0.527     0.379     0.441       103\n",
      "\n",
      "   micro avg      0.452     0.346     0.392       905\n",
      "   macro avg      0.230     0.177     0.196       905\n",
      "weighted avg      0.422     0.346     0.373       905\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/walter/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/walter/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/walter/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/walter/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/walter/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/walter/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Probar el modelo\n",
    "\n",
    "# Funciones utilitarias\n",
    "\n",
    "def max_values(x):\n",
    "    # Retorna el valor máximo y en índice o la posición del valor en un vector x.\n",
    "    # Parámetros: \n",
    "    #    x: vector con los datos. \n",
    "    # Salida: \n",
    "    #    out: valor \n",
    "    #    inds: índice\n",
    "    out, inds = torch.max(x,dim=1)   \n",
    "    return out, inds\n",
    "\n",
    "def test_examples(test_data):\n",
    "\n",
    "   with torch.no_grad():\n",
    "      inputs = prepare_sequence(test_data, word_to_ix)\n",
    "      tag_scores = model(inputs)\n",
    "        \n",
    "      _, pred = max_values(tag_scores)\n",
    "    \n",
    "#    print(\"Salida del modelo\", tag_scores)\n",
    "#    print(\"Valores máximos e índices\", max_values(tag_scores))    \n",
    "    \n",
    "   return pred.tolist()\n",
    "#    print(\"La frase original\", test_data)    \n",
    "#    print(\"La frase original preprocesada\", inputs)\n",
    "    \n",
    "\n",
    "#print(\"Índice de palabras\")\n",
    "#print(\"word_to_idx\", word_to_ix)\n",
    "\n",
    "print(\"Etiquetas\")\n",
    "# print(etiquetas_a_indice)\n",
    "\n",
    "#Frase 1\n",
    "# Las palabras en una oración se pueden etiquetar de tres formas.\n",
    "# La primera oración tiene 3 palabras \"El perro juega\"\n",
    "# por eso el tensor de salida tiene 3 elementos. \n",
    "# Cada elemento es un vector de pesos que indica cuál etiqueta tiene más\n",
    "# posibilidad de estar asociada a la palabra. Es decir hay que calcular \n",
    "# la posición del valor máximo. \n",
    "#   Ejemplo 1: \"El perro juega\" [\"DET\", \"NN\", \"V\"]\n",
    "# Ejemplo: 0, 1, 2 {\"DET\": 0, \"NN\": 1, \"V\": 2} => DET, NN, V \n",
    "y_pred = []\n",
    "y_test_procesed = []\n",
    "\n",
    "print(y_test[0])\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    y_pred.append(test_examples(X_test[i]))\n",
    "    y_test_procesed.append(prepare_sequence(y_test[i], etiquetas_a_indice).tolist())\n",
    "    \n",
    "# print(y_test[0])\n",
    "# print('-------')\n",
    "# test_examples(X_test[1])\n",
    "# print(y_test[1])\n",
    "# print('-------')\n",
    "# test_examples(X_test[2])\n",
    "# print(y_test[2])\n",
    "# print('-------')\n",
    "# test_examples(X_test[3])\n",
    "# print(y_test[3])\n",
    "# print('-------')\n",
    "# test_examples(X_test[4])\n",
    "# print(y_test[4])\n",
    "\n",
    "# [-3.1003e+00, -9.2929e+00, -3.9115e-01, -3.6556e+00, -7.8349e+00,\n",
    "#          -6.5042e+00, -8.1129e+00, -1.1431e+01, -1.5858e+00, -5.1965e+00,\n",
    "#          -7.3174e+00, -6.6276e+00, -7.0125e+00, -7.3155e+00, -7.7962e+00,\n",
    "#          -8.8214e+00, -3.3208e+00]\n",
    "\n",
    "# [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n",
    "\n",
    "sorted_labels = [k for k, v in etiquetas_a_indice.items()]\n",
    "print(sorted_labels, 'sorted_labels')\n",
    "labels_procesed = deepcopy(sorted_labels[1:])\n",
    "print(labels_procesed, 'labels_procesed')\n",
    "\n",
    "pred_labels = []\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    temp = []\n",
    "    for elem in y_pred[i]:\n",
    "        temp.append(sorted_labels[elem])\n",
    "    pred_labels.append(temp[:])\n",
    "\n",
    "# for elem in y_pred[index]:\n",
    "#     pred_labels.append(sorted_labels[elem])\n",
    "# print('sorted_labels', sorted_labels, len(sorted_labels))\n",
    "# print('---------------')\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if len(y_test[i]) != len(pred_labels[i]):\n",
    "        print('_________________________________', i)\n",
    "        print('y_test[i]', y_test[i], len(y_test[i]))\n",
    "        print('pred_labels[i]', pred_labels[i], len(pred_labels[i]))\n",
    "\n",
    "print('y_test', y_test[:3], len(y_test))\n",
    "\n",
    "print('---------------')\n",
    "print('pred_labels', pred_labels[:3], len(pred_labels))\n",
    "\n",
    "print(sklearn_crfsuite.metrics.flat_classification_report(y_test, pred_labels, labels=labels_procesed, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "unnecessary-dealing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'geo', 'start': 6, 'end': 6},\n",
       " {'label': 'geo', 'start': 12, 'end': 12},\n",
       " {'label': 'gpe', 'start': 18, 'end': 18}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "true = collect_named_entities(y_test[index])\n",
    "\n",
    "    \n",
    "# print(pred_labels)\n",
    "    \n",
    "pred = collect_named_entities(pred_labels[index])\n",
    "\n",
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2b4a340b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'geo', 'start': 6, 'end': 6},\n",
       " {'label': 'geo', 'start': 12, 'end': 12},\n",
       " {'label': 'gpe', 'start': 18, 'end': 18}]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "27a45bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'strict': {'correct': 3,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'precision': 0,\n",
       "   'recall': 0,\n",
       "   'f1': 0,\n",
       "   'actual': 3,\n",
       "   'possible': 3},\n",
       "  'ent_type': {'correct': 3,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'precision': 0,\n",
       "   'recall': 0,\n",
       "   'f1': 0,\n",
       "   'actual': 3,\n",
       "   'possible': 3},\n",
       "  'partial': {'correct': 3,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'precision': 0,\n",
       "   'recall': 0,\n",
       "   'f1': 0,\n",
       "   'actual': 3,\n",
       "   'possible': 3},\n",
       "  'exact': {'correct': 3,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'precision': 0,\n",
       "   'recall': 0,\n",
       "   'f1': 0,\n",
       "   'actual': 3,\n",
       "   'possible': 3}},\n",
       " {'geo': {'strict': {'correct': 2,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0,\n",
       "    'actual': 2,\n",
       "    'possible': 2},\n",
       "   'ent_type': {'correct': 2,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0,\n",
       "    'actual': 2,\n",
       "    'possible': 2},\n",
       "   'partial': {'correct': 2,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0,\n",
       "    'actual': 2,\n",
       "    'possible': 2},\n",
       "   'exact': {'correct': 2,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0,\n",
       "    'actual': 2,\n",
       "    'possible': 2}},\n",
       "  'gpe': {'strict': {'correct': 1,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0,\n",
       "    'actual': 1,\n",
       "    'possible': 1},\n",
       "   'ent_type': {'correct': 1,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0,\n",
       "    'actual': 1,\n",
       "    'possible': 1},\n",
       "   'partial': {'correct': 1,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0,\n",
       "    'actual': 1,\n",
       "    'possible': 1},\n",
       "   'exact': {'correct': 1,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0,\n",
       "    'actual': 1,\n",
       "    'possible': 1}}})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(true, pred, ['geo', 'gpe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f0abea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy \n",
    "\n",
    "metrics_results = {'correct': 0, 'incorrect': 0, 'partial': 0,\n",
    "                   'missed': 0, 'spurious': 0, 'possible': 0, 'actual': 0, 'precision': 0, 'recall': 0}\n",
    "\n",
    "# overall results\n",
    "results = {'strict': deepcopy(metrics_results),\n",
    "           'ent_type': deepcopy(metrics_results),\n",
    "           'partial':deepcopy(metrics_results),\n",
    "           'exact':deepcopy(metrics_results)\n",
    "          }\n",
    "\n",
    "labels_test = ['nat', 'geo', 'tim', 'eve', 'per', 'org', 'art', 'gpe']\n",
    "\n",
    "# results aggregated by entity type\n",
    "evaluation_agg_entities_type = {e: deepcopy(results) for e in labels_test}\n",
    "\n",
    "for true_ents, pred_ents in zip(y_test, pred_labels):\n",
    "#     print(true_ents)\n",
    "#     print(pred_ents)\n",
    "    \n",
    "    # compute results for one message\n",
    "    tmp_results, tmp_agg_results = compute_metrics(\n",
    "        collect_named_entities(true_ents), collect_named_entities(pred_ents),  labels_test\n",
    "    )\n",
    "    \n",
    "    #print(tmp_results)\n",
    "\n",
    "    # aggregate overall results\n",
    "    for eval_schema in results.keys():\n",
    "        for metric in metrics_results.keys():\n",
    "            results[eval_schema][metric] += tmp_results[eval_schema][metric]\n",
    "            \n",
    "    # Calculate global precision and recall\n",
    "        \n",
    "#     print('results', results)\n",
    "        \n",
    "    results = compute_precision_recall_wrapper(results)\n",
    "\n",
    "#     print('results', results)\n",
    "\n",
    "    # aggregate results by entity type\n",
    " \n",
    "    for e_type in labels_test:\n",
    "\n",
    "        for eval_schema in tmp_agg_results[e_type]:\n",
    "#             print(eval_schema)\n",
    "\n",
    "            for metric in tmp_agg_results[e_type][eval_schema]:\n",
    "#                 print(metric)\n",
    "                if metric == 'f1': # if el resultado va a ser 0 continue\n",
    "                    continue\n",
    "                evaluation_agg_entities_type[e_type][eval_schema][metric] += tmp_agg_results[e_type][eval_schema][metric]\n",
    "                \n",
    "        # Calculate precision recall at the individual entity level\n",
    "                \n",
    "        evaluation_agg_entities_type[e_type] = compute_precision_recall_wrapper(evaluation_agg_entities_type[e_type])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6c11e2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ent_type': {'correct': 264,\n",
       "  'incorrect': 120,\n",
       "  'partial': 0,\n",
       "  'missed': 227,\n",
       "  'spurious': 223,\n",
       "  'possible': 611,\n",
       "  'actual': 607,\n",
       "  'precision': 0.43492586490939045,\n",
       "  'recall': 0.4320785597381342,\n",
       "  'f1': 0.43349753694581283},\n",
       " 'partial': {'correct': 278,\n",
       "  'incorrect': 0,\n",
       "  'partial': 106,\n",
       "  'missed': 227,\n",
       "  'spurious': 223,\n",
       "  'possible': 611,\n",
       "  'actual': 607,\n",
       "  'precision': 0.5453047775947282,\n",
       "  'recall': 0.5417348608837971,\n",
       "  'f1': 0.5435139573070608},\n",
       " 'strict': {'correct': 210,\n",
       "  'incorrect': 174,\n",
       "  'partial': 0,\n",
       "  'missed': 227,\n",
       "  'spurious': 223,\n",
       "  'possible': 611,\n",
       "  'actual': 607,\n",
       "  'precision': 0.34596375617792424,\n",
       "  'recall': 0.3436988543371522,\n",
       "  'f1': 0.3448275862068965},\n",
       " 'exact': {'correct': 278,\n",
       "  'incorrect': 106,\n",
       "  'partial': 0,\n",
       "  'missed': 227,\n",
       "  'spurious': 223,\n",
       "  'possible': 611,\n",
       "  'actual': 607,\n",
       "  'precision': 0.45799011532125206,\n",
       "  'recall': 0.45499181669394434,\n",
       "  'f1': 0.45648604269293924}}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1a095fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nat': {'ent_type': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 1,\n",
       "   'spurious': 0,\n",
       "   'possible': 1,\n",
       "   'actual': 0,\n",
       "   'precision': 0,\n",
       "   'recall': 0.0,\n",
       "   'f1': 0},\n",
       "  'partial': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 1,\n",
       "   'spurious': 0,\n",
       "   'possible': 1,\n",
       "   'actual': 0,\n",
       "   'precision': 0,\n",
       "   'recall': 0.0,\n",
       "   'f1': 0},\n",
       "  'strict': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 1,\n",
       "   'spurious': 0,\n",
       "   'possible': 1,\n",
       "   'actual': 0,\n",
       "   'precision': 0,\n",
       "   'recall': 0.0,\n",
       "   'f1': 0},\n",
       "  'exact': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 1,\n",
       "   'spurious': 0,\n",
       "   'possible': 1,\n",
       "   'actual': 0,\n",
       "   'precision': 0,\n",
       "   'recall': 0.0,\n",
       "   'f1': 0}},\n",
       " 'geo': {'ent_type': {'correct': 92,\n",
       "   'incorrect': 23,\n",
       "   'partial': 0,\n",
       "   'missed': 49,\n",
       "   'spurious': 75,\n",
       "   'possible': 164,\n",
       "   'actual': 190,\n",
       "   'precision': 0.4842105263157895,\n",
       "   'recall': 0.5609756097560976,\n",
       "   'f1': 0.5197740112994351},\n",
       "  'partial': {'correct': 97,\n",
       "   'incorrect': 0,\n",
       "   'partial': 18,\n",
       "   'missed': 49,\n",
       "   'spurious': 75,\n",
       "   'possible': 164,\n",
       "   'actual': 190,\n",
       "   'precision': 0.5578947368421052,\n",
       "   'recall': 0.6463414634146342,\n",
       "   'f1': 0.5988700564971751},\n",
       "  'strict': {'correct': 82,\n",
       "   'incorrect': 33,\n",
       "   'partial': 0,\n",
       "   'missed': 49,\n",
       "   'spurious': 75,\n",
       "   'possible': 164,\n",
       "   'actual': 190,\n",
       "   'precision': 0.43157894736842106,\n",
       "   'recall': 0.5,\n",
       "   'f1': 0.4632768361581921},\n",
       "  'exact': {'correct': 97,\n",
       "   'incorrect': 18,\n",
       "   'partial': 0,\n",
       "   'missed': 49,\n",
       "   'spurious': 75,\n",
       "   'possible': 164,\n",
       "   'actual': 190,\n",
       "   'precision': 0.5105263157894737,\n",
       "   'recall': 0.5914634146341463,\n",
       "   'f1': 0.5480225988700566}},\n",
       " 'tim': {'ent_type': {'correct': 58,\n",
       "   'incorrect': 19,\n",
       "   'partial': 0,\n",
       "   'missed': 48,\n",
       "   'spurious': 33,\n",
       "   'possible': 125,\n",
       "   'actual': 110,\n",
       "   'precision': 0.5272727272727272,\n",
       "   'recall': 0.464,\n",
       "   'f1': 0.49361702127659574},\n",
       "  'partial': {'correct': 54,\n",
       "   'incorrect': 0,\n",
       "   'partial': 23,\n",
       "   'missed': 48,\n",
       "   'spurious': 33,\n",
       "   'possible': 125,\n",
       "   'actual': 110,\n",
       "   'precision': 0.5954545454545455,\n",
       "   'recall': 0.524,\n",
       "   'f1': 0.5574468085106382},\n",
       "  'strict': {'correct': 46,\n",
       "   'incorrect': 31,\n",
       "   'partial': 0,\n",
       "   'missed': 48,\n",
       "   'spurious': 33,\n",
       "   'possible': 125,\n",
       "   'actual': 110,\n",
       "   'precision': 0.41818181818181815,\n",
       "   'recall': 0.368,\n",
       "   'f1': 0.39148936170212767},\n",
       "  'exact': {'correct': 54,\n",
       "   'incorrect': 23,\n",
       "   'partial': 0,\n",
       "   'missed': 48,\n",
       "   'spurious': 33,\n",
       "   'possible': 125,\n",
       "   'actual': 110,\n",
       "   'precision': 0.4909090909090909,\n",
       "   'recall': 0.432,\n",
       "   'f1': 0.4595744680851064}},\n",
       " 'eve': {'ent_type': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'possible': 0,\n",
       "   'actual': 0,\n",
       "   'precision': 0,\n",
       "   'recall': 0,\n",
       "   'f1': 0},\n",
       "  'partial': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'possible': 0,\n",
       "   'actual': 0,\n",
       "   'precision': 0,\n",
       "   'recall': 0,\n",
       "   'f1': 0},\n",
       "  'strict': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'possible': 0,\n",
       "   'actual': 0,\n",
       "   'precision': 0,\n",
       "   'recall': 0,\n",
       "   'f1': 0},\n",
       "  'exact': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'possible': 0,\n",
       "   'actual': 0,\n",
       "   'precision': 0,\n",
       "   'recall': 0,\n",
       "   'f1': 0}},\n",
       " 'per': {'ent_type': {'correct': 48,\n",
       "   'incorrect': 16,\n",
       "   'partial': 0,\n",
       "   'missed': 39,\n",
       "   'spurious': 45,\n",
       "   'possible': 103,\n",
       "   'actual': 109,\n",
       "   'precision': 0.44036697247706424,\n",
       "   'recall': 0.46601941747572817,\n",
       "   'f1': 0.4528301886792453},\n",
       "  'partial': {'correct': 28,\n",
       "   'incorrect': 0,\n",
       "   'partial': 36,\n",
       "   'missed': 39,\n",
       "   'spurious': 45,\n",
       "   'possible': 103,\n",
       "   'actual': 109,\n",
       "   'precision': 0.42201834862385323,\n",
       "   'recall': 0.44660194174757284,\n",
       "   'f1': 0.4339622641509434},\n",
       "  'strict': {'correct': 23,\n",
       "   'incorrect': 41,\n",
       "   'partial': 0,\n",
       "   'missed': 39,\n",
       "   'spurious': 45,\n",
       "   'possible': 103,\n",
       "   'actual': 109,\n",
       "   'precision': 0.21100917431192662,\n",
       "   'recall': 0.22330097087378642,\n",
       "   'f1': 0.2169811320754717},\n",
       "  'exact': {'correct': 28,\n",
       "   'incorrect': 36,\n",
       "   'partial': 0,\n",
       "   'missed': 39,\n",
       "   'spurious': 45,\n",
       "   'possible': 103,\n",
       "   'actual': 109,\n",
       "   'precision': 0.25688073394495414,\n",
       "   'recall': 0.27184466019417475,\n",
       "   'f1': 0.2641509433962264}},\n",
       " 'org': {'ent_type': {'correct': 29,\n",
       "   'incorrect': 46,\n",
       "   'partial': 0,\n",
       "   'missed': 55,\n",
       "   'spurious': 52,\n",
       "   'possible': 130,\n",
       "   'actual': 127,\n",
       "   'precision': 0.2283464566929134,\n",
       "   'recall': 0.2230769230769231,\n",
       "   'f1': 0.22568093385214008},\n",
       "  'partial': {'correct': 49,\n",
       "   'incorrect': 0,\n",
       "   'partial': 26,\n",
       "   'missed': 55,\n",
       "   'spurious': 52,\n",
       "   'possible': 130,\n",
       "   'actual': 127,\n",
       "   'precision': 0.4881889763779528,\n",
       "   'recall': 0.47692307692307695,\n",
       "   'f1': 0.48249027237354086},\n",
       "  'strict': {'correct': 22,\n",
       "   'incorrect': 53,\n",
       "   'partial': 0,\n",
       "   'missed': 55,\n",
       "   'spurious': 52,\n",
       "   'possible': 130,\n",
       "   'actual': 127,\n",
       "   'precision': 0.1732283464566929,\n",
       "   'recall': 0.16923076923076924,\n",
       "   'f1': 0.17120622568093388},\n",
       "  'exact': {'correct': 49,\n",
       "   'incorrect': 26,\n",
       "   'partial': 0,\n",
       "   'missed': 55,\n",
       "   'spurious': 52,\n",
       "   'possible': 130,\n",
       "   'actual': 127,\n",
       "   'precision': 0.3858267716535433,\n",
       "   'recall': 0.3769230769230769,\n",
       "   'f1': 0.38132295719844356}},\n",
       " 'art': {'ent_type': {'correct': 0,\n",
       "   'incorrect': 2,\n",
       "   'partial': 0,\n",
       "   'missed': 5,\n",
       "   'spurious': 0,\n",
       "   'possible': 7,\n",
       "   'actual': 2,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0,\n",
       "   'f1': 0},\n",
       "  'partial': {'correct': 2,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 5,\n",
       "   'spurious': 0,\n",
       "   'possible': 7,\n",
       "   'actual': 2,\n",
       "   'precision': 1.0,\n",
       "   'recall': 0.2857142857142857,\n",
       "   'f1': 0.4444444444444445},\n",
       "  'strict': {'correct': 0,\n",
       "   'incorrect': 2,\n",
       "   'partial': 0,\n",
       "   'missed': 5,\n",
       "   'spurious': 0,\n",
       "   'possible': 7,\n",
       "   'actual': 2,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0,\n",
       "   'f1': 0},\n",
       "  'exact': {'correct': 2,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 5,\n",
       "   'spurious': 0,\n",
       "   'possible': 7,\n",
       "   'actual': 2,\n",
       "   'precision': 1.0,\n",
       "   'recall': 0.2857142857142857,\n",
       "   'f1': 0.4444444444444445}},\n",
       " 'gpe': {'ent_type': {'correct': 37,\n",
       "   'incorrect': 14,\n",
       "   'partial': 0,\n",
       "   'missed': 30,\n",
       "   'spurious': 18,\n",
       "   'possible': 81,\n",
       "   'actual': 69,\n",
       "   'precision': 0.5362318840579711,\n",
       "   'recall': 0.4567901234567901,\n",
       "   'f1': 0.4933333333333334},\n",
       "  'partial': {'correct': 48,\n",
       "   'incorrect': 0,\n",
       "   'partial': 3,\n",
       "   'missed': 30,\n",
       "   'spurious': 18,\n",
       "   'possible': 81,\n",
       "   'actual': 69,\n",
       "   'precision': 0.717391304347826,\n",
       "   'recall': 0.6111111111111112,\n",
       "   'f1': 0.6599999999999999},\n",
       "  'strict': {'correct': 37,\n",
       "   'incorrect': 14,\n",
       "   'partial': 0,\n",
       "   'missed': 30,\n",
       "   'spurious': 18,\n",
       "   'possible': 81,\n",
       "   'actual': 69,\n",
       "   'precision': 0.5362318840579711,\n",
       "   'recall': 0.4567901234567901,\n",
       "   'f1': 0.4933333333333334},\n",
       "  'exact': {'correct': 48,\n",
       "   'incorrect': 3,\n",
       "   'partial': 0,\n",
       "   'missed': 30,\n",
       "   'spurious': 18,\n",
       "   'possible': 81,\n",
       "   'actual': 69,\n",
       "   'precision': 0.6956521739130435,\n",
       "   'recall': 0.5925925925925926,\n",
       "   'f1': 0.6399999999999999}}}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_agg_entities_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "316950d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-24 17:29:43 root INFO: Imported 300 predictions for 300 true examples\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(y_test, pred_labels, labels_test)\n",
    "results, results_agg = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7f7613ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ent_type': {'correct': 224,\n",
       "  'incorrect': 70,\n",
       "  'partial': 0,\n",
       "  'missed': 319,\n",
       "  'spurious': 313,\n",
       "  'possible': 613,\n",
       "  'actual': 607,\n",
       "  'precision': 0.36902800658978585,\n",
       "  'recall': 0.36541598694942906},\n",
       " 'partial': {'correct': 278,\n",
       "  'incorrect': 0,\n",
       "  'partial': 16,\n",
       "  'missed': 319,\n",
       "  'spurious': 313,\n",
       "  'possible': 613,\n",
       "  'actual': 607,\n",
       "  'precision': 0.471169686985173,\n",
       "  'recall': 0.466557911908646},\n",
       " 'strict': {'correct': 210,\n",
       "  'incorrect': 84,\n",
       "  'partial': 0,\n",
       "  'missed': 319,\n",
       "  'spurious': 313,\n",
       "  'possible': 613,\n",
       "  'actual': 607,\n",
       "  'precision': 0.34596375617792424,\n",
       "  'recall': 0.3425774877650897},\n",
       " 'exact': {'correct': 278,\n",
       "  'incorrect': 16,\n",
       "  'partial': 0,\n",
       "  'missed': 319,\n",
       "  'spurious': 313,\n",
       "  'possible': 613,\n",
       "  'actual': 607,\n",
       "  'precision': 0.45799011532125206,\n",
       "  'recall': 0.4535073409461664}}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "45d32fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nat': {'ent_type': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 1,\n",
       "   'spurious': 313,\n",
       "   'possible': 1,\n",
       "   'actual': 313,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0},\n",
       "  'partial': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 1,\n",
       "   'spurious': 313,\n",
       "   'possible': 1,\n",
       "   'actual': 313,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0},\n",
       "  'strict': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 1,\n",
       "   'spurious': 313,\n",
       "   'possible': 1,\n",
       "   'actual': 313,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0},\n",
       "  'exact': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 1,\n",
       "   'spurious': 313,\n",
       "   'possible': 1,\n",
       "   'actual': 313,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0}},\n",
       " 'geo': {'ent_type': {'correct': 82,\n",
       "   'incorrect': 15,\n",
       "   'partial': 0,\n",
       "   'missed': 67,\n",
       "   'spurious': 313,\n",
       "   'possible': 164,\n",
       "   'actual': 410,\n",
       "   'precision': 0.2,\n",
       "   'recall': 0.5},\n",
       "  'partial': {'correct': 97,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 67,\n",
       "   'spurious': 313,\n",
       "   'possible': 164,\n",
       "   'actual': 410,\n",
       "   'precision': 0.23658536585365852,\n",
       "   'recall': 0.5914634146341463},\n",
       "  'strict': {'correct': 82,\n",
       "   'incorrect': 15,\n",
       "   'partial': 0,\n",
       "   'missed': 67,\n",
       "   'spurious': 313,\n",
       "   'possible': 164,\n",
       "   'actual': 410,\n",
       "   'precision': 0.2,\n",
       "   'recall': 0.5},\n",
       "  'exact': {'correct': 97,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 67,\n",
       "   'spurious': 313,\n",
       "   'possible': 164,\n",
       "   'actual': 410,\n",
       "   'precision': 0.23658536585365852,\n",
       "   'recall': 0.5914634146341463}},\n",
       " 'tim': {'ent_type': {'correct': 46,\n",
       "   'incorrect': 8,\n",
       "   'partial': 0,\n",
       "   'missed': 71,\n",
       "   'spurious': 313,\n",
       "   'possible': 125,\n",
       "   'actual': 367,\n",
       "   'precision': 0.12534059945504086,\n",
       "   'recall': 0.368},\n",
       "  'partial': {'correct': 54,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 71,\n",
       "   'spurious': 313,\n",
       "   'possible': 125,\n",
       "   'actual': 367,\n",
       "   'precision': 0.14713896457765668,\n",
       "   'recall': 0.432},\n",
       "  'strict': {'correct': 46,\n",
       "   'incorrect': 8,\n",
       "   'partial': 0,\n",
       "   'missed': 71,\n",
       "   'spurious': 313,\n",
       "   'possible': 125,\n",
       "   'actual': 367,\n",
       "   'precision': 0.12534059945504086,\n",
       "   'recall': 0.368},\n",
       "  'exact': {'correct': 54,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 71,\n",
       "   'spurious': 313,\n",
       "   'possible': 125,\n",
       "   'actual': 367,\n",
       "   'precision': 0.14713896457765668,\n",
       "   'recall': 0.432}},\n",
       " 'eve': {'ent_type': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 313,\n",
       "   'possible': 0,\n",
       "   'actual': 313,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0},\n",
       "  'partial': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 313,\n",
       "   'possible': 0,\n",
       "   'actual': 313,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0},\n",
       "  'strict': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 313,\n",
       "   'possible': 0,\n",
       "   'actual': 313,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0},\n",
       "  'exact': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 313,\n",
       "   'possible': 0,\n",
       "   'actual': 313,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0}},\n",
       " 'per': {'ent_type': {'correct': 32,\n",
       "   'incorrect': 5,\n",
       "   'partial': 0,\n",
       "   'missed': 67,\n",
       "   'spurious': 313,\n",
       "   'possible': 104,\n",
       "   'actual': 350,\n",
       "   'precision': 0.09142857142857143,\n",
       "   'recall': 0.3076923076923077},\n",
       "  'partial': {'correct': 28,\n",
       "   'incorrect': 0,\n",
       "   'partial': 9,\n",
       "   'missed': 67,\n",
       "   'spurious': 313,\n",
       "   'possible': 104,\n",
       "   'actual': 350,\n",
       "   'precision': 0.09285714285714286,\n",
       "   'recall': 0.3125},\n",
       "  'strict': {'correct': 23,\n",
       "   'incorrect': 14,\n",
       "   'partial': 0,\n",
       "   'missed': 67,\n",
       "   'spurious': 313,\n",
       "   'possible': 104,\n",
       "   'actual': 350,\n",
       "   'precision': 0.06571428571428571,\n",
       "   'recall': 0.22115384615384615},\n",
       "  'exact': {'correct': 28,\n",
       "   'incorrect': 9,\n",
       "   'partial': 0,\n",
       "   'missed': 67,\n",
       "   'spurious': 313,\n",
       "   'possible': 104,\n",
       "   'actual': 350,\n",
       "   'precision': 0.08,\n",
       "   'recall': 0.2692307692307692}},\n",
       " 'org': {'ent_type': {'correct': 27,\n",
       "   'incorrect': 29,\n",
       "   'partial': 0,\n",
       "   'missed': 75,\n",
       "   'spurious': 313,\n",
       "   'possible': 131,\n",
       "   'actual': 369,\n",
       "   'precision': 0.07317073170731707,\n",
       "   'recall': 0.20610687022900764},\n",
       "  'partial': {'correct': 49,\n",
       "   'incorrect': 0,\n",
       "   'partial': 7,\n",
       "   'missed': 75,\n",
       "   'spurious': 313,\n",
       "   'possible': 131,\n",
       "   'actual': 369,\n",
       "   'precision': 0.14227642276422764,\n",
       "   'recall': 0.40076335877862596},\n",
       "  'strict': {'correct': 22,\n",
       "   'incorrect': 34,\n",
       "   'partial': 0,\n",
       "   'missed': 75,\n",
       "   'spurious': 313,\n",
       "   'possible': 131,\n",
       "   'actual': 369,\n",
       "   'precision': 0.05962059620596206,\n",
       "   'recall': 0.16793893129770993},\n",
       "  'exact': {'correct': 49,\n",
       "   'incorrect': 7,\n",
       "   'partial': 0,\n",
       "   'missed': 75,\n",
       "   'spurious': 313,\n",
       "   'possible': 131,\n",
       "   'actual': 369,\n",
       "   'precision': 0.13279132791327913,\n",
       "   'recall': 0.37404580152671757}},\n",
       " 'art': {'ent_type': {'correct': 0,\n",
       "   'incorrect': 2,\n",
       "   'partial': 0,\n",
       "   'missed': 5,\n",
       "   'spurious': 313,\n",
       "   'possible': 7,\n",
       "   'actual': 315,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0},\n",
       "  'partial': {'correct': 2,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 5,\n",
       "   'spurious': 313,\n",
       "   'possible': 7,\n",
       "   'actual': 315,\n",
       "   'precision': 0.006349206349206349,\n",
       "   'recall': 0.2857142857142857},\n",
       "  'strict': {'correct': 0,\n",
       "   'incorrect': 2,\n",
       "   'partial': 0,\n",
       "   'missed': 5,\n",
       "   'spurious': 313,\n",
       "   'possible': 7,\n",
       "   'actual': 315,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0},\n",
       "  'exact': {'correct': 2,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 5,\n",
       "   'spurious': 313,\n",
       "   'possible': 7,\n",
       "   'actual': 315,\n",
       "   'precision': 0.006349206349206349,\n",
       "   'recall': 0.2857142857142857}},\n",
       " 'gpe': {'ent_type': {'correct': 37,\n",
       "   'incorrect': 11,\n",
       "   'partial': 0,\n",
       "   'missed': 33,\n",
       "   'spurious': 313,\n",
       "   'possible': 81,\n",
       "   'actual': 361,\n",
       "   'precision': 0.10249307479224377,\n",
       "   'recall': 0.4567901234567901},\n",
       "  'partial': {'correct': 48,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 33,\n",
       "   'spurious': 313,\n",
       "   'possible': 81,\n",
       "   'actual': 361,\n",
       "   'precision': 0.1329639889196676,\n",
       "   'recall': 0.5925925925925926},\n",
       "  'strict': {'correct': 37,\n",
       "   'incorrect': 11,\n",
       "   'partial': 0,\n",
       "   'missed': 33,\n",
       "   'spurious': 313,\n",
       "   'possible': 81,\n",
       "   'actual': 361,\n",
       "   'precision': 0.10249307479224377,\n",
       "   'recall': 0.4567901234567901},\n",
       "  'exact': {'correct': 48,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 33,\n",
       "   'spurious': 313,\n",
       "   'possible': 81,\n",
       "   'actual': 361,\n",
       "   'precision': 0.1329639889196676,\n",
       "   'recall': 0.5925925925925926}}}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-court",
   "metadata": {},
   "source": [
    "5. Genere   y   documente   sus   conclusiones   (incluya   al   menos   cuatro conclusiones importantes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-vietnamese",
   "metadata": {},
   "source": [
    "Referencias\n",
    "----------------\n",
    "*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
